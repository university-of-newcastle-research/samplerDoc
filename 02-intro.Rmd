# PMwG sampler and sequential sampling models


In this chapter we'll demonstrate how to use the PMwG sampler with a sequential sampling model; the Linear Ballistic Accumulator (LBA).
Please ensure the pmwg package is installed. We currently recommended installing pmwg via devtools.
```{r getpkg, eval=FALSE}
# The pmwg package will be on CRAN - this step will be removed.
install_github('newcastlecl\pmwg')
```
```{r loadpkg}
library(pmwg)
```

## Description of Forstmann experiment

Forstmann et al looked at neural correlates of decision making under time pressure, with an aim to identify areas of the brain associated with speed-accuracy tradeoff. Imaging (fMRI) and behavioural data was collected; however, we will analyse behavioural data from the decision-making task only. In terms of modelling the data, Forstmann expected to find differences in thresholds (direction?) for each of the three speed-emphasis conditions. We have included the Forstmann et als data in the pmwg package as a data frame object named `forstmann`. The sampler requires a data frame with a <b>`subject`</b> column. The subject column data type can be a factor or numeric.

Table \@ref(tab:forsthead10) shows the first ten trials from the Forstmann dataset. Participants `(n = 19)` were asked to indicate whether a cloud of dots in a random-dot kinematogram (RDK) moved to the left or the right of the screen. The IV was a within-subject, speed-accuracy manipulation where, before each trial began, pariticipants were instructed to make their choice <i>accurately</i> `(condition = 1)`, with <i>urgency</i>`(condition = 3)`or were presented with a <i>neutral</i> message `(condition = 2)`. Stimuli moved either <i>left</i> `(stim = 1)` or <i>right</i> `(stim = 2)` and responses <i>left</i> `(resp = 1)` or <i>right</i> `(resp = 2)`. Response times `(rt)` were recorded in seconds. For more information about the design of the experiment please see [the original paper](https://www.pnas.org/content/105/45/17538.short).

```{r forsthead10, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
kable(head(forstmann, 10), row.names = F,  align = "ccccc", caption = 'First 10 trials in Forstmann dataset. The `forstmann` dataset is an object/data frame ')
```

## Linear Ballistic Accumulator Parameters

There are preliminary steps we need to complete before running the sampler. Let's begin by defining the Linear Ballistic Accumulator (LBA) [@brown2008simplest] model parameters.

* `b` threshold parameter (the evidence required to trigger a response)
* `v` is drift rate or average speed of evidence accumulation
* `A` is the model's start point 
* `t0` is non-decision time 
* `sv` is the standard deviation of drift rate

## Writing the log-likelihood function

Just as we did with the SDT example, we'll write a slow and a fast log-likelihood function. The runtime difference is caused by calling the `dLBA` function line-by-line for the slow log-likelihood and calling the `dLBA` function once for all the data in the fast log-likelihood function. When writing a new log-likelihood function, we suggest starting with a slow, line-by-line function for easier debugging. 


The LBA log-likelihood function takes three arguments: 

* `x` is a named parameter vector (e.g. `pars`)
* `data` is your data set (e.g.`forstmann`). Your dataset must include a `"subject"` column
* `sample = FALSE` calculates a density function or `TRUE` generates a posterior, predictive sample that matches the shape of data.


The log-likelihood function shown below includes functions from the `rtdists` package for generating data and estimating density. If you'd like to run through this example, it is best to copy the `tw_lba_ll` function from the code block below rather than copying from the separate code chunks where curly braces have been removed.

NOTE: The trialwise log-likelihood is very slow and inneficient because `rLBA` and `dLBA` will be called on each line of the data. This will result in very slow sampling times and is a consequence of the `rtdists` package, not an issue with the PMwG sampling speed. If you have experience writing log-likelihoods, we recommend writing a faster version than our trialwise function, or use the fast log-likelihood we have written in section \@ref(fstLBALL). If you are new to modelling, we recommend trying the trialwise (slow) log-likelihood function as it is easier to follow, troubleshoot and is less likely to result in errors. 

Let's begin by installing `rtdists` package...

```{r instrtdist}
library(rtdists)
```

and now our complete trialwise (slow) log-likelihood function.

```{r trialwiseLL, attr.source = '.numberLines', include=TRUE }
tw_lba_ll <- function(x, data, sample = FALSE) {
  x <- exp(x)
  if (any(data$rt < x["t0"])) {
    return(-1e10)
  }
  
  if (sample) {
    tmp <- numeric(nrow(data))
    data$rt <- rep(NA, nrow(data))
    data$resp <- rep(NA, nrow(data))
  } else {
    out <- numeric(nrow(data))
  }
  
  for (i in 1:nrow(data)) {
    A = x["A"]
    b = x[paste0("b.", data$condition[i])] + A
    vc = x["vc"]
    ve = x["ve"]
    t0 = x["t0"]
    s = c(1, 1)
    
    if (data$stim[i] == 1) {
      vs = c(vc, ve)
      } else {
      vs = c(ve, vc)
      }
    
    if (sample) {
      tmp <- rLBA(n = 1,
                  A = A,
                  b = b,
                  mean_v = vs,
                  sd_v = s,
                  t0 = t0,
                  dist = "norm",
                  silent = TRUE
                  )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    } else {
      out[i] <- dLBA(rt = data$rt[i],
                     response = data$resp[i],
                     A = A,
                     b = b,
                     mean_v = vs,
                     sd_v = s,
                     t0 = t0,
                     dist = "norm",
                     silent = TRUE
      )
      }
  }
  
if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
}
```


The first line in the `tw_lba_ll` function (Line 2 below) takes the exponent of the parameter values. We do this as the LBA expects values on the real number line. Line 3 and 4 then checks RTs are faster than the non-decision time parameter `t0`, and return a low value, indicating that the given value of `t0` is not likely. 

```{r twLBAll1, attr.source = '.numberLines startFrom="2"', echo=TRUE, eval=FALSE}
  x <- exp(x)
  if (any(data$rt < x["t0"])) {
    return(-1e10)
```


Now we create a vector with values sampled from the posterior distribution OR estimating the density. If `sample = TRUE`, we remove all responses `(resp)` and rts.  This means when we return `data`, we are returning the posterior predictive data which matches with the associated `subject` and `condition`.

If `sample = FALSE` (the `else` statement from line 11) we create an `out` vector, with a length equal to the number of rows in the dataset, and store the likelihood value for each subject and condition.

```{r twLBAll2, attr.source = '.numberLines startFrom="7"', echo=TRUE, eval=FALSE}
  if (sample) {
    tmp <- numeric(nrow(data))
    data$rt <- rep(NA, nrow(data))
    data$resp <- rep(NA, nrow(data))
  } else {
    out <- numeric(nrow(data))
  }
```


Next, we loop over rows in the datset. In this `for` loop, we find the values `(x)` of each parameter in our model for each row, so that any conditional parameters (for example `b` in our model) are correctly assigned. For example, we want to calculate the density for a model that has three threshold parameters (one for each condition; `1  = accuracy`, `2 = neutral`, or `3 = speed`). In the loop, we paste `b.` to the condition in row `[i]` and add `A` (the start point - we do this to ensure the threshold is greater than the starting point). 

On line 22 we set the correct order of our drift rate parameters. Recall that `stim = 1` is a stimulus moving to the left. Therefore, when `data$stim[i] == 1`, the drift rate for the correct accumulator `(vc)` is first -- so we order the drift rates (i.e. Vs or `vs` in the code); `vs = c(vc, ve)`. The `else` statement takes care of right moving stimuli `data$stim[i] == 2`, the incorrect accumulator `(ve)` is the first accumulator, so the drift rate parameter order is `vs = c(ve, vc)`. This ensures that the correct `(vc)` and error `(ve)` drift rates align with the correct stimulus/response ????? for that condition. 

```{r twLBAll3, attr.source = '.numberLines startFrom="15"', echo=TRUE, eval=FALSE}
  for (i in 1:nrow(data)) {
    A = x["A"]
    b = x[paste0("b.", data$condition[i])] + A
    vc = x["vc"]
    ve = x["ve"]
    t0 = x["t0"]
    s = c(1, 1)
    if (data$stim[i] == 1) {
      vs = c(vc, ve)
    } else {
      vs = c(ve, vc)
    }
```

The following section calls the relevant `rtdists` function depending on whether we are sampling from the posterior `(rLBA)` or estimating the density `(dLBA)`. We then input the parameters from above (using the names set above) to the relevant function. When generating data from the posterior (Line 30-37), `rLBA` is called for each line of the data, storing the generated `rt` and `response` given the posterior parameter estimates in the `tmp` vector (which we then reassign to the empty `data$rt` and `data$resp` columns). We set `n = 1`, since we are calling `rLBA` on 1 row of the data. When estimating the density (Line 42 to 50), dLBA is called for each line of the data, storing the probability of the `rt` and `response` under the proposed parameters `(x)` in the `out` vector. 


From line 23 to 25 in the code block above, we reversed the order of the correct and incorrect drift rate parameters. We can now explain this with reference to line (....) in the else section above.  If `stim = 1` (left) - the first accumulator is `vc` (correct) and the second accumulaor is `ve` (incorrect).  If `resp[i] = 1` (left response), then this tells the sampler that the first accumulator was chosen (the correct accumulator). If `resp[i] = 2` (right response), this tells the sampler that the second (incorrect) accumulator was chosen. Now lets flip this, so that `data$stim[i] == 2` (right) as per the else statement on line (...). Now the drfit rate for the incorrect accumulator `(ve)` is first, and the drift rate for the correct accumulator `(vc)` is second.   If `data$resp[i] == 1` (left response), then this tells the sampler that the first accumulator was chosen (the incorrect accumulator `ve`). If `data$resp[i] == 2` (right response), this tells the sampler that the second accumulator (correct accumulator `vc`) was chosen. 

```{r lbaLL3, attr.source='.numberLines startFrom="29"', echo=TRUE, eval=FALSE}
  if (sample) {
      tmp <- rLBA(n = 1,
                  A = A,
                  b = b,
                  mean_v = vs,
                  sd_v = s,
                  t0 = t0,
                  dist = "norm",
                  silent = TRUE
                  )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    } else {
      out[i] <- dLBA(rt = data$rt[i],
                     response = data$resp[i],
                     A = A,
                     b = b,
                     mean_v = vs,
                     sd_v = s,
                     t0 = t0,
                     dist = "norm",
                     silent = TRUE
      )
      }
```

This final section tells the function what to return; `data` - when sampling posterior predictive (Line 56) - or the sum of the likelihoods - when estimating density (Line x - y). On line 58 we take all implausible likelihood values, assign them to the `bad` object and then (line 59) set them to zero. The final two lines within the `else` statement take the log of all likelihood values, sums them, assigns the modelâ€™s log-likelihood value to the `out` variable and returns that value.

```{r lbaLL43, attr.source='.numberLines startFrom="55"', echo=TRUE, eval=FALSE}
if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
```


### Fast LBA Loglikelihood Function {#fstLBALL}

As the data is large, and the dlba takes time to run, the function above is computationally inefficient. Below, we show a faster log-likelihood function. There are several methods of making the log-likelihood faster; in the current example, we reduce the number of calls to dlba (and rlba when sampling) to one call. We do this by passing a list of dlba parameter values for the length of the data. 

NOTE: in the example below, we don't change the function to sample from the posterior. When generating posterior predictive data, we only call the function several times, so computational efficiency is not as important. Running this line by line gives us a "safe" method of generating posterior predictive data. 






Now we know the LBA model parameters, we can create a vector of model parameter names, which we'll use in our log-likelihood function. You can name this object as you wish; however, in our example, we will call it `pars`. The number of parameters and the parameter names you include in the `pars` vector <b>must</b> match those included in your log-likelihood function.
```{r pars}
pars <- c("b1", "b2", "b3", "A", "v1", "v2", "t0")
```

For the Forstmann dataset, we need three threshold parameters (`b1`, `b2`, and `b3` i.e. one for each condition) because we assume response caution differs for each verbal instruction. We include two drift rate parameters: `v1` for the incorrect accumulator and `v2` for the correct accumulator, a start point parameter `A` and a non-decision time `t0` parameter.  We've made a decision to set the `sv` to 1 to satisfy the scaling properties of the model. We haven't included the `sv` parameter in the `pars` vector, however it is found in the LBA's likelihood function  below.

Next we create a `priors` object; a list that contains two components <b> Do we need to explain what priors are and why we do this?</b>

* `start_mu` a vector containing the prior for model parameter means
* `start_sig` the prior covariance matrix for model parameters.

```{r priors}
priors <- list(start_mu = rep(0, length(pars)),
  start_sig = diag(rep(1, length(pars)))
)
```
The `priors` object in our example is initiated with zeros. <b>Under what conditions would this priors object differ?</b>






The next step is to include your log-likelihood function. This must be called before you create the sampler object in the following step. You can load your log-likelihood function from an external script:
```{r loadLL, echo=TRUE, eval=FALSE}
source(file = "yourLogLikelihoodFile.R")
```



Once you've setup your parameters, priors and your log-likelihood function, the next step is to initialise the `sampler` object. 
```{r samplerObject, echo=TRUE, eval=FALSE}
sampler <- pmwgs(
  data = forstmann,
  pars = pars,
  prior = priors,
  ll_func = lba_loglike
)
```

The `pmwgs` function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps.

* `data =` your data - a data frame (e.g.`forstmann`) with a column for participants called <b>`subject`</b>
* `pars =` the model parameters to be used (e.g.`pars`)
* `prior =` the priors to be used (e.g.`priors`)
* `ll_func =` name of log-likelihood function you've sourced above (e.g.`lba_loglike`)

### Model start points {#start-points}
You have the option to set model start points. We have specified sensible start points for the Forstmann dataset. If you chose not to specify start points, the sampler will randomly sample points from the prior distribution.
```{r modStartPoints, echo=TRUE, eval=FALSE}
start_points <- list(
  mu = c(.2, .2, .2, .4, .3, 1.3, -2),
  sig2 = diag(rep(.01, length(pars)))
)
```

The `start_points` object contains two vectors:

* `mu` a vector of start points for the mu of each model parameter 
* `sig2` vector containing the start points of the covariance matrix of covariance between model parameters.


### Running the sampler {#run-sampler}
Okay - now we are ready to run the sampler.  
```{r runSampler, echo=TRUE, eval=FALSE}
sampler <- init(sampler, theta_mu = start_points$mu,
                theta_sig = start_points$sig2)
```
Here we are using the `init` function to generate initial start points for the random effects and storing them in the `sampler` object.  First we pass the `sampler` object from above that includes our data, parameters, priors and log-likelihood function.  If we decided to specify our own start points (as above), we would include the `theta_mu` and `theta_sig` arguments.


Now we can run the sampler using the `run_stage` function. The `run_stage` function takes four arguments:

* `x` the `sampler` object including parameters 
* `stage =` the sampling stage (e.g. `"burn"`, `"adapt"` or `"sample"`)
* `iter = ` is the number of iterations for the sampling stage 
* `particles =` is the number of particles generated on each iteration 

It is optional to include the `iter =` and `particles =` arguments. If these are not included, `iter` and `particles` default to 1000. The number of iterations you choose for your burn in stage is similar to choices made when running deMCMC, however, this varies depending on the time the model takes to reach the 'real' posterior space. 

First we run our burn in stage by setting `stage =` to `"burn"`. Here we have set iterations to be 500, which may take some time. 
```{r burn, echo=TRUE, eval=FALSE}
burned <- run_stage(sampler, stage = "burn", iter = 500, particles = 1000)
```

Now we run our adaptation stage by setting `stage = "adapt"` Because we have not specified number of iterations or particles, the sampler will use the default value of 1000 for each of these arguments. N.B. The sampler will quit adaptation stage after 20 unique values have been accepted for each subject. This means adaptation may not use all 1000 iterations.
```{r adaptation, echo=TRUE, eval=FALSE}
adapted <- run_stage(burned, stage = "adapt")
```


At the start of the `sampled` stage, the sampler object will create a 'proposal' distribution for each subject's random effects using a conditional multi-variate normal. This proposal distribution is then used to efficiently generate new particles for each subject which means we can reduce the number of particles on each iteration whilst still achieving acceptance rates.
```{r sampled, echo=TRUE, eval=FALSE}
sampled <- run_stage(adapted, stage = "sample", iter = 100, particles = 100)
```








