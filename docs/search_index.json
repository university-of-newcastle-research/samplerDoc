[["index.html", "Particle Based Samplers for MCMC Chapter 1 Introduction to particle based sampler for MCMC 1.1 Assumed knowledge 1.2 Why would you use Particle Metropolis within Gibbs sampling? 1.3 The assumed hierarchical structure 1.4 What Particle Metropolis within Gibbs sampling provides 1.5 What is Particle Metropolis within Gibbs sampling? 1.6 Generating proposals in PMwG sampling using Particle Metropolis", " Particle Based Samplers for MCMC Chapter 1 Introduction to particle based sampler for MCMC Contains implementations of particle based sampling methods for model parameter estimation. Primarily an implementation of the Particle Metropolis within Gibbs sampler outlined in the paper available here, it also contains code for covariance estimation. 1.1 Assumed knowledge To get the most out of this tutorial for the PMwG sampler, we assume you are familiar with hierarchical Bayesian estimation and MCMC sampling. If you would like to read more on these topics, please see Shiffrin et al. (2008) tutorial on Hierachical Bayesian Methods and Van Ravenzwaaij, Cassey, and Brown (2018) introduction to Markov Chain Monte-Carlo Sampling. 1.2 Why would you use Particle Metropolis within Gibbs sampling? This software is intended to help estimate models in a hierarchical structure with random effects for subjects. You will need to be able to write a function that evaluates the density of one subject’s data, given values for that subject’s parameters (i.e. their random effects). Everything else is taken care of by the sampler functions. The model that defines the density for an individual subject’s data could be a regression model, a simple cognitive model like signal detection models (which is one of the examples we cover here), or models that can be very challenging to estimate, such as the Linear Ballistic Accumulator (LBA) or the Drift Diffusion model. As long as you have a model for which you can provide a likelihood, the PMwG software will help estimate the model in a hierarchical Bayesian way. Benefits of the Particle Metropolis within Gibbs sampling algorithm include: It allows you to efficiently get posterior samples from difficult-to-estimate models with highly correlated parameters, such as the LBA or diffusion model, and these samples have nice properties (e.g., lower autocorrelation than other MCMC samplers). Statistical efficiency makes it feasible to draw a large number of posterior samples. This can be important in posterior inference, for example in calculating Bayes Factors using established methods. It allows you to estimate the covariance structure between parameters in a principled manner. 1.3 The assumed hierarchical structure The PMwG package is very flexible in that it is agnostic about the data-level model; it allows the user to specify the model that defines the density of each subject’s data. However, the package makes fixed assumptions about the hierarchical structure across participants. The package assumes a multivariate normal random effect structure. For example, when estimating an LBA model, each participant will have several parameters, such as a start point (A), threshold (b), drift rate (v), and non-decision time (t0). The PMwG package assumes that the vector of each subject’s parameter value follows a group-level distribution which is multivariate normal. The algorithm will estimate the group-level mean for each parameter, as well as its variance, and also the correlations between parameters in the sample. One consequence of the multivariate normal assumption is that all parameters are assumed to be unbounded (i.e. able to take values anywhere on the real line). Cognitive models often have bounded parameters (e.g. in the LBA model, the non-decision time parameter cannot be negative, as it represents a length of time). The user should deal with bounded parameters by transforming them to be unbounded. We give examples of that, in the likelihood functions. 1.4 What Particle Metropolis within Gibbs sampling provides The sampler will provide samples from the posterior distribution of the model in three categories: The means for the group level parameters (theta). The vectors of random effects for each subject (individual level parameter values, alpha) The group-level variance covariance matrix (sigma). 1.5 What is Particle Metropolis within Gibbs sampling? There are two sampling approaches incorporated into PMwG. One is the well-established and easy to apply Gibbs sampling on the group-level parameters. Gibbs sampling is very powerful and efficient, and it can work for the group-level parameters because the package assumes a multivariate normal distribution, which is easy to work with. However, for the subject-level parameters (random effects), Gibbs sampling is not possible; at least not for most cognitive models. For this reason, the PMwG package uses particle methods to sample random effects. Particle sampling works like other Markov chain samplers, such as Metropolis-Hastings. At each step, the sample (the vector of random effects) from the previous step is compared to a large number of proposals (“particles”). The new sample is chosen from amongst the particles (including the previous sample), according to how well they match the data and the prior. The key to making the algorithm efficient is to propose particles carefully. Our algorithm uses adaptive proposal distributions, individually tuned for each participant, to make sure that good proposal particles are generated without requiring a prohibitively large number of particles in total. How often the sampler accepts a new particle (compared to the previous sample) is referred to as the acceptance rate. Acceptance rate can be adjusted for maximum efficiency (somewhere around 30-50% acceptance is great) by changing the total number of particles and by changing the variance of the proposal distribution (parameter “epsilon”). The particles are evaluated in parallel, which increases computation speed. 1.6 Generating proposals in PMwG sampling using Particle Metropolis PMwG has three sampling stages (demarcated by the black, vertical lines in the plot below). The first stage is called burn-in, the second is adaptation, and the third is the sampled stage. The stages employ different numbers of particles, and different proposal distributions. This makes the algorithm most efficient. Figure 1.1: Trace plot with vertical lines demarcating the PMwG sampler’s three sampling stages In the burn-in and adaptation stages, the proposals (or random effects vectors) for each subject are sampled from a mixture of two sources. One source is the group-level distribution and the second source is a multivariate normal distribution centred on the current best guess for the subject’s random effects vector (alpha), with a variance that is smaller than the group level distribution. We generate proposals from both sources, because proposals generated from the group level distribution act as a safety net in situations where proposals generated from the subject’s current alpha are unusual or unlikely. If this occurs instead of the sampler taking a long way to return a sensible vector of random effects, a group level proposal will instead be chosen, leading to a faster sampling time. One thing we want to know is the posterior distribution for each subject’s random effects (alpha). For this reason, we throw away samples from the burn-in stage, because this is a period in which the sampler is trying to move away from an initial guess, and stabilise on samples which are from the true posterior distribution for each subject’s random effects. In the adaptation stage, we continue the algorithm used in the burn-in stage until we collect a minimum of 20 unique samples from each subject’s posterior distribution. These samples are used to give a good idea of what the posterior distribution looks like for each subject’s random effects vector. That allows us to build an adaptive proposal distribution that makes very efficient proposals, in the next stage. In the final sampling stage (sample), we generate a multivariate normal distribution (referred to as the adaptive proposal distribution), that summarises the unique samples in the adaptation stage, and use this to generate future proposals. An important feature of this distribution is that, for each subject, it summarises not only the posterior distribution of their random effects but also the way these random effects relate to the group-level parameter. This allows us to draw conditional proposals; proposals which are both consistent with that subject’s random effects and with the current proposal for the group-level distribution. Because of this, the proposals generated are frequently accepted, so we can lower the number of particles needed in this stage (for example, 20 instead of 200), and still maintain an adequate acceptance rate. Further, we continue to update this proposal distribution throughout the sample stage so we have a more accurate proposal distribution. As a safety precaution during the sample stage, we also include a few proposal particles from the same algorithm as used in the burn-in stage. This protects against very poor conditional proposal distributions. References Shiffrin, Richard M, Michael D Lee, Woojae Kim, and Eric-Jan Wagenmakers. 2008. “A Survey of Model Evaluation Approaches with a Tutorial on Hierarchical Bayesian Methods.” Cognitive Science 32 (8): 1248–84. Van Ravenzwaaij, Don, Pete Cassey, and Scott D Brown. 2018. “A Simple Introduction to Markov Chain Monte–Carlo Sampling.” Psychonomic Bulletin &amp; Review 25 (1): 143–54. "],["pmwg-sampler-and-signal-detection-theory.html", "Chapter 2 PMwG sampler and Signal Detection Theory 2.1 Testing the SDT log-likelihood function 2.2 SDT log-likelihood function for Wagenmakers experiment 2.3 PMwG Framework 2.4 Check the sampling process 2.5 Simulating posterior data", " Chapter 2 PMwG sampler and Signal Detection Theory Here we demonstrate how to use the PMwG sampler package to run a simple signal detection theory (SDT) analysis on data from a lexical decision task. We recognise that it is unnecessary to use the sampler package for a simple analysis such as this; however, we hope this SDT example demonstrates the practicality of the PMwG sampler package. 2.0.1 Signal Detection Theory analysis of lexical decision task We won’t cover SDT and lexical decision tasks in detail here; however, we will explain how you can use the PMwG package with SDT in the context of a lexical decision task. If you require more information, please visit the SDT and lexical decision task Wikipedia pages. Also, this Frontiers in Psychology tutorial paper by Anderson (2015) is another good resource for SDT. Participants were asked to indicate whether a letter string was a word or a non-word. We begin with the distributions for non-word and word stimuli. You can think of these two distributions as the ‘noise’ and ‘signal’ curves, respectively. Each distribution represents the evidence for ‘word-likeness’ and they are assumed to be normally distributed. The non-word distribution (or the ‘noise’ distribution) has a mean (\\(\\mu\\)) of 0 and a standard deviation (SD) of 1. We could estimate SD here, but we will use 1 in this example for simplicity. The mean for the word distribution is unknown at this point; however, we assign a d-prime (d’) parameter to denote the difference between the mean of the non-word and the mean of the word distributions (i.e. the ‘sensitivity’ to word stimuli or the signal-noise difference between words and non-word). As can be seen in Figure 2.1, the word distribution mean is greater than the non-word distribution mean; however, the distributions partially overlap where non-words and words are difficult to classify. Figure 2.1: Simple SDT example of lexical decision task The second parameter we denote is the criterion (C) parameter. The criterion is the point at which an individual responds non-word (to the left of C in Figure 2.2) or word (to the right of C in Figure 2.2) and it is set somewhere between the means of the two distributions. If you’re biased to respond word, the criterion would move to the left. Conversely, if you’re biased to respond non-word then the criterion would move to the right. Figure 2.2: Simple SDT example of lexical decision task 2.0.2 The log-likelihood function 2.0.2.1 What is a log-likelihood? The log-likelihood function is the ‘engine’ of the PMwG sampler. It is the way that the parameters of the model are connected to the data. In making this connection between data and parameters, the log-likelihood function actually defines the model and it’s associated psychological theory. The function takes in data (for a single subject) parameter values (for a single subject - ``random effects’’) and calculates the likelihood of the data given those random effect values. For most data sets, this process operates line-by-line: the log-likelihood is calculated for each observation and added. For example, we could compute the likelihood of a response time given a set of random effect values for a model. If the data are more likely given these random effect values are more likely (i.e. the model is closer to the data), the likelihood will be higher. In the likelihood function, the parameter values must be assigned to the correct conditions and model parameters. For example, a linear regression model could have a slope and intercept parameter for easy and hard conditions i.e. four parameters. In this case, we would need to map the input parameter value for the slope in the easy condition onto the easy condition’s slope model parameter and then do the same for the hard slope and intercept parameters. This is much of the work of the log-likelihood function. Once mapped, we can use the appropriate values to calculate how likely were the data observed in each condition. Then we can calculate how likely the data are for the subject. 2.0.2.2 Writing a simple log-likelihood function Let’s write a simple log-likelihood function for a fabricated data set. You can copy the code below to follow along with our example. stim &lt;- c(&quot;word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;non-word&quot;, &quot;non-word&quot;, &quot;word&quot;) resp &lt;- c(&quot;word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;non-word&quot;, &quot;word&quot;, &quot;non-word&quot;) fab_data &lt;- as.data.frame(cbind(stim, resp)) We create our dataset by combining a response resp and a stimulus stim vector into a data frame as shown in 2.1 below. Table 2.1: A fabricated dataset of 8 trials with a response and a stimuls column stim resp word word word word non-word non-word word word non-word non-word non-word non-word non-word word word non-word Our log-likelihood function will step through the data, line by line, and find a likelihood value for each trial, under two parameters; d-prime d and criterion C. Here is our complete log-likelihood function. We have omitted some code from the code blocks below to enhance their appearance, so we encourage you to copy the log-likelihood function from the following code block if you’d like to follow along with our example. We’ll now step through each component of the log-likelihood below. SDT_ll &lt;- function(x, data, sample = FALSE){ if (!sample) { out &lt;- numeric(nrow(data)) for (i in 1:nrow(data)) { if (stim[i] == &quot;word&quot;) { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } sum(out) } } We initialise the log-likelihood function with three arguments SDT_ll &lt;- function(x, data, sample = FALSE) { x is a named parameter vector (e.g.pars) data is the dataset sample = sample values from the posterior distribution (For this simple example, we do not require a sample argument. ) The first if statement (line 2) checks if you want to sample, this is used for posterior predictive sampling which we will cover in later chapters. If you’re not sampling (like us in this example), you need to create an output vector out. The out vector will contain the log-likelihood value for each row/trial in your dataset. if (!sample) { data$response &lt;- NA } else { out &lt;- numeric(nrow(data)) } From line 9, we check each row in the data set, first considering all trials with word stimuli if (stim[i] == \"word\" (line 10), and assign a likelihood for responding word (line 12-13) or non-word (line 15-16). The word distribution has a mean of x[\"d\"] (d-prime parameter) and a decision criterion parameter x[\"C\"]. If the response is word, we are considering values ABOVE or to the right of C in figure 2.2, so we set lower.tail = to FALSE. If the response is non-word, we look for values BELOW or to the left of C in figure 2.2 and we set lower.tail = to TRUE. The log.p = argument takes the log of all likelihood values when set to TRUE. We do this so we can sum all likelihoods at the end of the log-likelihood function. if (!sample) { for (i in 1:nrow(data)) { if (stim[i] == &quot;word&quot;) { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } From the else statement on line 18, we have the function for non-word trials i.e. stim[i] == \"non-word\". As can be seen below, the output value out[i] for these trials is arrived at in a similar manner to the word trials. We set the mean to 0 and the standard deviation sd to 1. If the response is word, we are considering values ABOVE or to the right of C in figure 2.2, so we set lower.tail = to FALSE. If the response is non-word, we look for values BELOW or to the left of C in figure 2.2 and we set lower.tail = to TRUE. Again we want the log of all likelihood values so we set log.p = TRUE. else { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } The final line of code on line 24 sums the out vector and returns a log-likelihood value for your model. sum(out) 2.1 Testing the SDT log-likelihood function Before we run the log-likelihood function, we must create a parameter vector pars containing the same parameter names used in our log-likelihood function above i.e. we name the criterion C and d-prime parameter d. While we’re testing the log-likelihood, we assign arbitrary values to each parameter. pars &lt;- c(C = 0.8, d = 2) We can test run our log-likelihood function by passing the parameter vector pars and the fabricated dataset we created above fab_data. SDT_ll(pars, fab_data) ## [1] -4.795029 Now, if we change the parameter values, the log-likelihood value should also change. pars &lt;- replace(pars, c(1,2), c(0.5, 1.2)) SDT_ll(pars, fab_data) ## [1] -4.532791 We can see the log-likelihood has changed. The second vector of parameter values are more likely than the first vector given the data. 2.2 SDT log-likelihood function for Wagenmakers experiment Now that we’ve covered a simple test example, let’s create a log-likelihood function for the Wagenmakers et al. (2008) dataset. 2.2.1 Description of Wagenmakers experiment If you’d like to follow our example, you will need to download the dataset from this link.The structure of the dataset will need to be modified in order to meet the requirements of the PMwG sampler. To do this, you can copy our script. You may attempt to modify the dataset yourself, but you must recreate the structure illustrated in table 2.2. Participants were asked to indicate whether a letter string was a word or a non-word. A subset of Wagenmakers et al data are shown in table 2.2, with each line representing a single trial. We have a subject column with a subject id (1-19), a condition column cond which indicates the proportion of words to non-words presented within a block of trials. In word blocks (cond = w) participants completed 75% word and 25% non-word trials and for non-word (cond = nw) blocks 75% non-word and 25% word trials. The stim column lists the word’s frequency i.e. is the stimulus a very low frequency word (stim = vlf), a low frequency word (stim = lf), a high frequency word (stim = hf) or a non-word (stim = nw). The third column resp refers to the participant’s response i.e. the participant responded word (resp = W) or non-word (resp = NW). The two remaining columns list the response time (rt) and whether the paricipant made a correct (correct = 2) or incorrect (correct = 1) choice. For more details about the experiment please see the original paper. Table 2.2: Subset of 12 trials from the Wagenmakers (2008) dataset. subject condition stim resp rt correct 1 w lf W 0.410 2 1 w hf W 0.426 2 1 w nw NW 0.499 2 1 w lf W 0.392 2 1 w vlf W 0.435 2 1 w hf W 0.376 2 1 nw lf W 0.861 2 1 nw hf W 0.563 2 1 nw nw NW 0.666 2 1 nw nw NW 1.561 2 1 nw nw NW 0.503 2 1 nw nw NW 0.445 2 Our log-likelihood function for Wagenmakers experimental data is similar to the function we wrote above, except now we require a criterion parameter for each condition and a d-prime parameter for each of the stim word types. This is illustrated in figure 2.3 below, where we have a non-word criterion Cnw, a word criterion Cw and three distributions for each of the stim types with corresponding d-prime for each distribution: dvlf, dlf, dhf. Figure 2.3: Signal detection theory example of lexical decision task Here is our complete log-likelihood function for the Wagenmakers data set. SDT_ll &lt;- function(x, data, sample = FALSE){ if (sample){ data$response &lt;- NA } else { out &lt;- numeric(nrow(data)) } if (!sample){ for (i in 1:nrow(data)) { if (data$cond[i] == &quot;w&quot;) { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;){ if (data$resp[i] == &quot;W&quot;){ out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } else { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } } sum(out) } } Line 1 through to line 8 are the same as the log-likelihood we wrote for the fabricated dataset above. From line 9, we calculate the log-likelihood out[i] for word condition trials cond[i] == \"w\" when the stimulus is a high frequency word stim[i] == \"hf\" for each response. We do this by considering the upper tail of the high frequency word distribution lower.tail = FALSE, from the word criterion Cw, for word responses resp[i] == \"W\" and the lower tail for non-word responses (else statement on line 14). We then recycle this process for the remaining conditions/parameters in the experiment. if (data$cond[i] == &quot;w&quot;) { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } This give us a log-likelihood for all data. Let’s test this… pars &lt;- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7)) SDT_ll(pars, wgnmks2008, sample = FALSE) ## [1] -30801.71 2.2.2 Computation time of the log-likelihood function You may have noticed that our log-likelihood function is slow and heavy on computer time when processing the data trial by trial. We recommend you write a ‘slow’ log-likelihood (as written above) to check it functions as it should before improving the function’s efficiency. Now we’ll speed up our log-likelihood function. We have 16 possible values that could be assigned per line in the previous function (for the 16 cells of the design, given by proportion (2) x stimuli (4) x response (2)). Rather than looping over each trial, we could calculate the log-likelihood for each cell in the design and multiply the number of instances for each subject. To do this, we add a “n” column to the dataframe as shown in the code below. wgnmks2008Fast &lt;- as.data.frame(table(wgnmks2008$subject, wgnmks2008$cond, wgnmks2008$stim, wgnmks2008$resp)) names(wgnmks2008Fast) &lt;- c(&quot;subject&quot;, &quot;cond&quot;, &quot;stim&quot;, &quot;resp&quot;, &quot;n&quot;) Now our data frame looks like this.. subject cond stim resp n 1 nw hf NW 1 2 nw hf NW 2 3 nw hf NW 11 4 nw hf NW 0 5 nw hf NW 6 6 nw hf NW 9 For our SDT log-likelihood function, we add a multiplying factor (data$n[i]) to calculate the model log-likelihood and rather than looping over trials. SDT_ll_fast &lt;- function(x, data, sample = FALSE) { if (!sample) { out &lt;- numeric(nrow(data)) for (i in 1:nrow(data)) { if (data$cond[i] == &quot;w&quot;) { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } }else{ if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;){ if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } } sum(out) } } Now we have a fast(er) SDT log-likelihood function and we can compare its output with the slow log-likelihood function’s output to make sure it is functioning correctly. pars &lt;- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7)) SDT_ll(pars, wgnmks2008, sample = FALSE) ## [1] -30801.71 SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE) ## [1] -30801.71 Great—both functions produce the same log-likelihood! And we can run one final check by modifying the parameter vector’s values and seeing if the log-likelihood value updates. pars &lt;- log(c(C.w = 1, C.nw = 0.8, HF.d = 2.7, LF.d = 1.8, VLF.d = 1.3)) SDT_ll(pars, wgnmks2008, sample = FALSE) ## [1] -22168.95 SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE) ## [1] -22168.95 As we saw with the fabricated dataset, the log-likelihood has changed. The second vector of parameter values are more likely than the first vector, given the data. We recommend speeding up your code however you wish. When you’re confident that your log-likelihood functions correctly, you should save it as a separate script so it can be sourced and loaded when running the sampler. 2.3 PMwG Framework Now that we have written a log-likelihood function, we’re ready to use the PMwG sampler package. Let’s begin by loading the PMwG package. library(pmwg) Now we require the parameter vector pars we specified above and a priors object called priors. The priors object is a list that contains two components: theta_mu_mean a vector that is the prior for the mean of the group-level mean parameters theta_mu_var a covariance matrix that is the prior for the variance of the group-level mean parameters. In all examples we assume a diagonal matrix. pars &lt;- c(&quot;C.w&quot;, &quot;C.nw&quot;, &quot;HF.d&quot;, &quot;LF.d&quot;, &quot;VLF.d&quot;) # This is the same as the `pars` vector specified above priors &lt;- list( theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(1, length(pars))) ) The priors object in our example is initiated with zeros. An important thing to note is that to facilitate Gibbs sampling from the multivariate normal distribution for the group parameters, the random effects must be estimated on the real line. In our SDT example, the parameters are free to vary along the real line so no transformation of the random effects is required. We expand on this point in more detail in later examples where parameters are bounded and require transformation. The prior on the covariance matrix is hard-coded as the marginally non-informative prior of Huang and Wand, as discussed in the PMwG paper. The next step is to load your log-likelihood function/script. source(file = &quot;yourLogLikelihoodFile.R&quot;) Once you have set up your parameters, priors and written a log-likelihood function, the next step is to initialise the sampler object. sampler &lt;- pmwgs(data = wgnmks2008Fast, pars = pars, prior = priors, ll_func = SDT_ll_fast ) The pmwgs function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps. data =a data frame (e.g.wgnmks2008Fast) with a column for participants called subject pars = the model parameters to be used (e.g.pars) prior = the priors to be used (e.g.priors) ll_func = name of log-likelihood function you’ve sourced above (e.g.SDT_ll_fast) 2.3.1 Model start points You have the option to set model start points. We use 0 for the mean (mu) and a variance (sig2) of 0.01. If you chose not to specify start points, the sampler will randomly sample points from the prior distribution. start_points &lt;- list(mu = rep(0, length.out = length(pars)), sig2 = diag(rep(.01, length(pars))) ) The start_points object contains two vectors: mu a vector of start points for the mean the model parameters sig2 vector containing the start points of the covariance matrix of covariance between model parameters. 2.3.2 Running the sampler Okay - now we are ready to run the sampler. sampler &lt;- init(sampler, start_mu = start_points$mu, start_sig = start_points$sig2) Here we are using the init function to generate initial start points for the random effects and storing them in the sampler object. First we pass the sampler object from above that includes our data, parameters, priors and log-likelihood function. If we decided to specify our own start points (as above), we would include the start_mu and start_sig arguments. Now we can run the sampler using the run_stage function. The run_stage function takes four arguments: x the sampler object including parameters that was created from the init function above. stage = the sampling stage (e.g. \"burn\", \"adapt\" or \"sample\") iter = is the number of iterations for the sampling stage. This is similar to running deMCMC, where it takes many iterations to reach the posterior space. Default = 1000. particles = is the number of particles generated on each iteration. Default = 1000. display_progress = shows progress bar for current stage n_cores = the number of cores to be used when running the sampler epsilon = is a value between 0 and 1 which reduces the size of the sampling space. We use lower values of epsilon when there are more parameters to estimate. Default = 1. It is optional to include the iter = and particles = arguments. If these are not included, iter and particles default to 1000. The number of iterations you choose for your burn-in stage is similar to choices made when running deMCMC; however, this varies depending on the time the model takes to reach the ‘real’ posterior space. First we run our burn-in stage by setting stage = to \"burn\". burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 1000, particles = 20, display_progress = TRUE, n_cores = 8) Now we run our adaptation stage by setting stage = \"adapt\". This function creates an efficient proposal distribution. The sampler will attempt to create the proposal distribution after 20 unique particles have been accepted for each subject. The sampler will then test whether the distribution was able to be created and if it was created, the sampler will move to the next stage otherwise the sampler will continue to sample. The number of iterations needs to be great enough to generate enough unique samples. The sampler will automatically exit the adapt stage when it has enough unique samples to create a multivariate normal ‘proposal’ distribution for each subject’s random effects. Thus we set iterations to a high number, as it should exit before reaching this point. adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;, iter = 1000, particles = 20, n_cores = 8) At the start of the sampled stage, the sampler object will create a ‘proposal’ distribution for each subject’s random effects using a conditional multi-variate normal. This proposal distribution is then used to efficiently generate new particles for each subject which means we can reduce the number of particles on each iteration whilst still achieving acceptance rates. sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 1000, particles = 20, n_cores = 8) 2.4 Check the sampling process It is a good idea to check your samples by producing some simple plots as shown below. The first plot gives an indication of the trace for the group level parameters. In this example, you will see the chains take only several iterations before arriving at the posterior; however, this may not always be the case. Each parameter trace (horizontal lines on plot 2.4) should be stationary i.e. the trace should not trend up or down, and once the sampler reaches the posterior, the trace should remain relatively ‘thin’. If the trace is wide and bounces between large values (e.g. between -3 and 3) then there may be an error in your log-likelihood function. As you can see in 2.4, the traces are clearly stable. Note that the number of iterations for the adaptation stage here is quite short because it’s easy to estimate and so gets lots of unique draws. Note: The sampling stages (i.e. burn-in, adapatation and sampling) are demarcated by the black, vertical lines. Figure 2.4: Posterior samples of parameters The second plot below (figure 2.5) shows the likelihoods across iterations for each subject. Again we see that the likelihood values jump up after only a few iterations and then remain stable, with only slight movement. Figure 2.5: Posterior samples of subject log-likelihoods 2.5 Simulating posterior data Now we’ll cover the sample operation within the fast log-likelihood function. We will use this on the full data set. The sample operation can be carried out in several ways (using rbinom etc). Please note that we do NOT recommend using this approach below and this should serve as an example only. The sample process is similar to what we’ve covered above. We begin by assigning NAs to the response column to prepare it for simulated response data. We then consider a subset of the data, beginning with word condition and high-frequency hf word stimuli trials. else{ data$resp &lt;- NA for (i in 1:nrow(data)){ if (data$cond[i] == &quot;w&quot;){ if (data$stim[i] == &quot;hf&quot;){ We then take the criterion for the word condition i.e. C.w. To simulate a response given our parameters we use rnorm to pick a random value from a normal distribution with mean = HF.d (i.e. high frequency word stimulus) and a SD of 1 and we test that value against the word criterion C.w. If the value is larger than C.w, the simulated response will be word otherwise, the simulated response will be non-word. data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;HF.d&quot;], sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) We repeat this process for each condition and stimulus combination as shown in the code block below. { else if (data$stim[i] == &quot;lf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;LF.d&quot;], sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) } else if (data$stim[i] == &quot;vlf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;VLF.d&quot;], sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) } else { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = 0, sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) } } else { if (data$stim[i] == &quot;hf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;HF.d&quot;], sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } else if (data$stim[i] == &quot;lf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;LF.d&quot;], sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } else if (data$stim[i] == &quot;vlf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;VLF.d&quot;], sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } else { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = 0, sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } } Now we can run our simulation. Below is some code to achieve this. n.posterior &lt;- 20 # Number of samples from posterior distribution for each parameter. pp.data &lt;- list() S &lt;- unique(wgnmks2008$subject) data &lt;- split(x = wgnmks2008, f = wgnmks2008$subject) for (s in S) { cat(s,&quot; &quot;) iterations = round(seq(from = 1051, to = sampled$samples$idx, length.out = n.posterior)) for (i in 1:length(iterations)) { x &lt;- sampled$samples$alpha[, s, iterations[i]] names(x) &lt;- pars tmp &lt;- SDT_ll_fast(x = x, data = wgnmks2008[wgnmks2008$subject == s,], sample = TRUE) if (i == 1) { pp.data[[s]] &lt;- cbind(i,tmp) } else { pp.data[[s]] &lt;- rbind(pp.data[[s]], cbind(i, tmp)) } } } And now we can plot samples against the data. Figure 2.6: The proportion of ‘word’ responses within each cell of the design. Figure 2.6 shows 20 posterior draws (dots) plotted against the data (bars). The posterior draws are for each individual subject - shown here is the average proportion of word responses. We can see that the model fits the data well. References Anderson, Nicole D. 2015. “Teaching Signal Detection Theory with Pseudoscience.” Frontiers in Psychology 6: 762. Wagenmakers, Eric-Jan, Roger Ratcliff, Pablo Gomez, and Gail McKoon. 2008. “A Diffusion Model Account of Criterion Shifts in the Lexical Decision Task.” Journal of Memory and Language 58 (1): 140–59. "],["forstmannChapter.html", "Chapter 3 PMwG sampler and sequential sampling models 3.1 The speed-accuracy trade-off in perceptual decisions 3.2 Linear Ballistic Accumulator Parameters 3.3 The log-likelihood function 3.4 PMwG Framework 3.5 Simulating Posterior Predictive Data 3.6 Checking Descriptive Adequacy of 1b model. 3.7 Model Comparison 3.8 Checking the LBA log-likelihood function", " Chapter 3 PMwG sampler and sequential sampling models In this chapter we’ll demonstrate how to use the PMwG sampler with a sequential sampling model; the Linear Ballistic Accumulator (LBA). Please ensure the PMwG and rtdists packages are loaded. library(pmwg) library(rtdists) require(tidyverse) 3.1 The speed-accuracy trade-off in perceptual decisions We demonstrate the application of the LBA with the PMwG sampler in a study of perceptual decision making. Forstmann et al. (2008) looked at neural correlates of decision making under time pressure, with an aim to identify areas of the brain associated with speed-accuracy trade-off. Imaging (fMRI) and behavioural data was collected; however, we will analyse behavioural data from the decision-making task only. In terms of modelling the data, Forstmann expected to find differences in thresholds for each of the three speed-emphasis conditions. We have included the data from Forstmann et al in the PMwG package as a data frame named forstmann. The sampler requires a data frame with a subject column. The subject column data type can be a factor or numeric. Table 3.1 shows the first ten trials from the Forstmann dataset. Participants (n = 19) were asked to indicate whether a cloud of dots in a random-dot kinematogram moved to the left or the right of the screen. The IV was a within-subject, speed-accuracy manipulation where, before each trial began, participants were instructed to make their choice accurately (condition = 1), with urgency(condition = 3)or were presented with a neutral message (condition = 2). Stimuli moved either left (stim = 1) or right (stim = 2) and responses were left (resp = 1) or right (resp = 2). Response times (rt) were recorded in seconds. For more information about the design of the experiment please see the original paper. Table 3.1: First 10 trials in Forstmann dataset. The forstmann dataset is a data frame subject condition stim resp rt 1 1 2 2 0.4319 1 3 2 2 0.5015 1 3 1 1 0.3104 1 1 2 2 0.4809 1 1 1 1 0.3415 1 2 1 1 0.3465 1 2 1 1 0.3572 1 2 2 2 0.4042 1 2 1 1 0.3866 1 1 2 2 0.3683 3.2 Linear Ballistic Accumulator Parameters There are preliminary steps we need to complete before running the sampler. Let’s begin by defining the Linear Ballistic Accumulator (LBA) (Brown and Heathcote 2008) model parameters. b threshold parameter (the evidence required to trigger a response) v is the mean drift rate or average speed of evidence accumulation A is the range of start points for accumulators t0 is non-decision time sv is the standard deviation of the across-trial distribution of drift rates 3.3 The log-likelihood function 3.3.1 What is a log-likelihood function? If you’re unsure what a log-likelihood function is and/or does, see our explanation here. 3.3.2 Writing the log-likelihood function for the Forstmann data set Just as we did with the SDT chapter, we will write a slow and a fast log-likelihood function. The trialwise (slow) log-likelihood function is approximately five times slower than the fast log-likelihood function because the dLBA function is called line-by-line (trialwise), where as the dLBA function is called once for all the data in the fast log-likelihood function. When writing a new log-likelihood function, we suggest starting with a slow, line-by-line function for easier debugging. See section 3.8 for a detailed debugging process. The LBA log-likelihood function takes three arguments: x is a named parameter vector (e.g. pars) data is your dataset (e.g.forstmann). Your dataset must include a \"subject\" column sample = FALSE calculates a density function or TRUE generates a posterior predictive sample that matches the shape of data. The log-likelihood function shown below includes functions from the rtdists package for generating data and estimating density. If you’d like to run through this example, it is best to copy the tw_lba_ll function from the code block below rather than copying from the separate code chunks where curly braces have been removed. Note: The trialwise log-likelihood is very slow and inefficient because rLBA and dLBA will be called on each line of the data. This will result in very slow sampling times and is a consequence of the rtdists package, not an issue with the PMwG sampling speed. If you have experience writing log-likelihoods, we recommend writing a faster version than our trialwise function, or use the fast log-likelihood we have written in section 3.3.3. If you are new to modelling, we recommend trying the trialwise (slow) log-likelihood function as it is easier to follow, troubleshoot and is less likely to result in errors. Let’s begin by loading the rtdists package… library(rtdists) and now our complete trialwise (slow) log-likelihood function. tw_lba_ll &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { tmp &lt;- numeric(nrow(data)) data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[paste0(&quot;b.&quot;, data$condition[i])] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } if (sample) { tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } else { out[i] &lt;- dLBA(rt = data$rt[i], response = data$resp[i], A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) } } if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } } The first line in the tw_lba_ll function (Line 2 below) takes the exponent of the parameter values. We do this as the LBA requires positive parameter values that are on the real line. Line 3 and 4 then checks whether response times (RTs) are faster than the non-decision time parameter t0, and returns a low value if t0 is larger than RT, indicating that the given value of t0 is unlikely. x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) Now we create a vector with values sampled from the posterior distribution OR estimating the density. If sample = TRUE, we remove all responses (resp) and RTs. This means when we return data, we are returning the posterior predictive data which matches with the associated subject and condition and then generate them from the random function of the model. If sample = FALSE (the else statement from line 11) we create an out vector, with a length equal to the number of rows in the dataset, and store the likelihood value for each subject and condition. if (sample) { tmp &lt;- numeric(nrow(data)) data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } Next, we loop over rows in the dataset. In this for loop, we find the values (x) of each parameter in our model for each row, so that any conditional parameters (for example b in our model) are correctly assigned. For example, we want to calculate the density for a model that has three threshold parameters (one for each condition; 1 = accuracy, 2 = neutral, or 3 = speed). In the loop, we paste b. to the condition in row [i] and add A (the start point - we do this to ensure the threshold is greater than the starting point). On line 22 we set the order of our drift rate parameters. Recall that stim = 1 is a stimulus moving to the left. dLBA requires the drift accumulators to be matched i.e. when data$stim[i] == 1, the drift rate for the correct accumulator (vc) is in position one, so we order the drift rates; vs = c(vc, ve). The else statement addresses right moving stimuli data$stim[i] == 2, the incorrect accumulator (ve) is the first accumulator, so the drift rate parameter order is vs = c(ve, vc). This ensures that the correct (vc) and error (ve) drift rates match with the corresponding accumulators for given stimuli. for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[paste0(&quot;b.&quot;, data$condition[i])] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } The following section calls the relevant rtdists function depending on whether we are sampling from the posterior predictive distribution (rLBA) or estimating the density (dLBA). We then input the parameters from above (using the names set above) into the relevant function. When generating data from the posterior predictive distribution (Line 30-37), rLBA is called for each line of the data, storing the generated rt and response given the posterior parameter estimates in the tmp vector (which we then reassign to the empty data$rt and data$resp columns). We set n = 1, since we are calling rLBA on 1 row of the data. When estimating the density (Line 42 to 50), dLBA is called for each line of the data, storing the probability of the rt and response under the proposed parameters (x) in the out vector. if (sample) { tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } else { out[i] &lt;- dLBA(rt = data$rt[i], response = data$resp[i], A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) } This final section tells the function what to return; data - when sampling posterior predictive data (Line 56) - or the sum of the likelihoods - when estimating density (Line 57 - 61). On line 58 we take all implausible likelihood values, assign them to the bad object and then (line 59) set them to an extremely unlikely value, to prevent numerical errors. The final two lines within the else statement take the log of all likelihood values, sums them, assigns the model’s log-likelihood value to the out variable and returns that value. if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } 3.3.3 Fast LBA Log-likelihood Function As the data is large, and the dLBA function takes some time to run, the log-likelihood code above is computationally inefficient. There are several ways to improve the log-likelihood’s performance; in our example below, we reduce the number of calls to dLBA to one call. We do this by passing a list of dLBA parameter values for the length of the data. Note: When generating posterior predictive data, the rLBA function is still executed for each row of data; however, it is only executed several times, so computational efficiency is not compromised. fast_lba_ll3b &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } if (sample) { for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[paste0(&quot;b.&quot;, data$condition[i])] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } } else { all_b &lt;- numeric(nrow(data)) vlist &lt;- list(&quot;v.1&quot; = numeric(nrow(data)), &quot;v.2&quot; = numeric(nrow(data))) stim &lt;- levels(data$stim) con &lt;- levels(data$condition) for (c in con) { for (s in stim) { use &lt;- data$condition == c &amp; data$stim == s if (any(use)) { bs = x[paste0(&quot;b.&quot;, c)] + x[&quot;A&quot;] all_b[use] = bs vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] if (s == 1) { vlist$v.1[use] = vc vlist$v.2[use] = ve } else { vlist$v.1[use] = ve vlist$v.2[use] = vc } } } } out &lt;- dLBA(rt = data$rt, response = data$resp, A = x[&quot;A&quot;], b = all_b, mean_v = vlist, sd_v = c(1, 1), t0 = x[&quot;t0&quot;], distribution = &quot;norm&quot;, silent = TRUE ) } if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } } You should improve your log-likelihood’s performance as you see fit. When you’re confident that your log-likelihood code functions correctly, we suggest saving it as a separate script so it can be sourced and loaded when running the sampler. If you’re learning how to write log-likelihood functions, take a look at our troubleshooting section for tips. In our experience, a very large proportion of problems with sampling and inference are caused by inadequate checking and care in the likelihood function. 3.4 PMwG Framework Now that we have a log-likelihood function, we can set up the PMwG sampler. Running the sampler follows the same procedure outlined in the SDT chapter; we need to set up a vector of model parameter names, create a priors object, source our LBA log-likelihood script and then create our sampler object. Let’s begin by creating a vector of model parameter names, which we’ll use in our log-likelihood function. You can name this object as you wish; however, in our example, we name it pars. 1 For the forstmann dataset, we use three threshold parameters (one b for each condition) because we assume that the condition has an effect on level of caution, we include two drift rate parameters: ve for the incorrect accumulator and vc for the correct accumulator, a start point parameter A and a non-decision time t0 parameter. We’ve made a decision to set the sv to 1 to satisfy the scaling properties of the model. As such, we haven’t included the sv parameter in the pars vector - it is found in the LBA’s log-likelihood function (see above). pars &lt;- c(&quot;b.1&quot;, &quot;b.2&quot;, &quot;b.3&quot;, &quot;A&quot;, &quot;ve&quot;, &quot;vc&quot;, &quot;t0&quot;) For the mean of the distribution for random effects (theta_mu), we assume a multivariate normal prior. The user can specify the mean and variance of this prior distribution using the object priors, which has elements theta_mu_mean and theta_mu_var. A typical setting for LBA models is to set theta_mu_mean to be a zero vector and to set theta_mu_var to be a multiple of the identity matrix, e.g. with 9 on the diagonal (representing a standard deviation of 3 for the subject-level means in the prior). We create our priors object; a list that contains two components: theta_mu_mean a vector containing the prior for model parameter means theta_mu_var the prior covariance matrix for model parameters. priors &lt;- list(theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(1, length(pars))) ) Now source and load your log-likelihood script before you create the sampler object. source(file = &quot;dataObjects/fast_lba_ll3b.R&quot;) Next we specify the PMwG sampler object. The pmwgs function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps. data = your data - a data frame (e.g.forstmann) with a column for participants called subject pars = the model parameters to be used (e.g.pars) prior = the priors to be used (e.g.priors) ll_func = name of log-likelihood function to be used (e.g.fast_lba_ll3b) sampler &lt;- pmwgs(data = data, pars = pars, prior = priors, ll_func = fast_lba_ll3b ) 3.4.1 Model start points There is also an option to set model start points. We have specified sensible start points for the forstmann dataset under the LBA model. If you choose not to specify start points, the sampler will randomly sample points from the prior distribution. The start_points object contains two vectors: mu a vector of start points for the mu of each model parameter sig2 vector containing the start points of the covariance matrix of covariance between model parameters. Note: Start points must be on the real line. Our log-likelihood function immediately takes the exponent of the start points and only returns positive values, so we use the log of sensible start points here. start_points &lt;- list(mu = log(c(1.2, 1.2, 1.2, 1.4, 1.3, 3.5, 0.13) ), sig2 = diag(rep(.01, length(pars))) ) 3.4.2 Running the sampler Setup is now complete and we can run the sampler. First, we use the init function to generate initial start points for the random effects and store them in the sampler object. Here we specify start points by providing values for the start_mu and start_sig arguments. Note: The init stage can take some time to run because it uses a large number of particles sampler &lt;- init(sampler, start_mu = start_points$mu, start_sig = start_points$sig2 ) To run the sampler, we use the run_stage function. To execute the run_stage function, you must provide values for two arguments: pmwgs = the sampler object including parameters that were created by the init function above. stage = the sampling stage (In order; \"burn\", \"adapt\" or \"sample\"). The following arguments listed below are optional: iter = is the number of iterations for the sampling stage. For burn-in, it is important to have enough iterations that the chains converge on the posterior distribution. Default = 1000. particles = is the number of proposals (particles) generated for each random effect, on each iteration. Default = 1000, but set smaller or larger in order to target a reasonable acceptance rate (i.e. 10-60%). display_progress = display a progress bar during sampling epsilon = is a value greater than 0 which scales the variance of the proposal distribution. Smaller values (i.e. narrower proposal distributions) can lead to higher acceptance rates, but slower coverage of the posterior. Smaller values are especially useful when the number of random effects is large (e.g. &gt;10). The default is adaptively chosen based on the number of parameters. n_cores = the number of cores on a machine you wish to use to run the sampler. This allows sampling to be run across cores (parallelising for subjects). Default = 1. Note: Setting n_cores greater than 1 is only permitted on Linux and Mac OS X machines. The first sampling stage is burn-in \"burn\". The burn-in stage allows time for the sampler to move from the (arbitrary) start points that were provided by the user to the mode of the posterior distribution. We take the sampler object created in the init function above, set the stage argument to \"burn\" and assign the outcome to an object called burned. Note: You should visually check the chains for convergence/stationarity after burn-in. burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 1000, particles = 100, epsilon = .5 ) Next is the adaptation stage \"adapt\". The adaptation stage draws samples using a simple, but relatively inefficient proposal distribution (the same proposal distribution as the \"burn\"stage). Enough samples are drawn to allow the algorithm to estimate a much more sophisticated and efficient proposal distribution, using conditional normal distributions. We take the burned object created in the previous stage and set iterations iter = to a high number (e.g. 10000), as it should exit before reaching this point. If it doesn’t, there is likely an issue with acceptance rates, the likelihood function or limited data to operate on (i.e. few trials in some conditions). Here, we have saved the outcome of the adaptation stage to an object called adapted. adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;, iter = 10000, particles = 100, epsilon = .5 ) The final stage is the sampling stage \"sample\". The sampling stage uses the sophisticated and adaptive conditional normal proposal distributions. This allows for very efficient sampling, using far fewer particles. Samples from this stage are taken from the ‘posterior distribution’ and stored in the sampled object. sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 10000, particles = 100, epsilon = .5 ) The sampled object includes all samples from the \"sample\" stage above and the following elements: data : the data (data frame) you included in your analysis par_names: parameter names n_pars: number of parameters n_subjects: number of subjects subjects: subject IDs (1:n) prior: list of the prior used ll_func: the likelihood function specified samples: alpha: three dimensional array of random effects draws (dim = parameters x subjects x samples) theta_mu: two dimensional array of parameter draws (dim = parameters x samples) theta_sig: three dimensional array of covariance matrix draws (dim = covariance x samples) stage: specifies the stage the sample is from (length = samples) subj_ll: likelihood value for each subject for each iteration (dim = subject x samples) a_half: the parameter used in calculating the inverse Wishart (dim = parameters x samples) idx: total number of samples last_theta_sig_inv: the inverse of the last sample for theta_sig (the variance-covariance matrix). You should save your sampled object at this point. save(sampled, file = &quot;forst3bSamp.RData&quot;) 3.4.3 Determining estimation settings for the PMwG sampler Deciding on values for iterations and particles (and epsilon) is entirely model and data dependent. As model complexity increases, so to does the number of particles required. Increasing the number of particles gives the PMwG sampler a greater chance of finding a new particle on each iteration. For epsilon, as the number of parameters increases, the epsilon value decreases because smaller epsilon values restrict the sampling range/space. As a result, reaching the posterior space is slower, however, it means that we generate more particles closer to the current particle, which increases the amount of new particles accepted (i.e. acceptance rate). For the iterations, this also links in with number of particles and the value of epsilon. The main aim of the initial stages is to reach the posterior space and generate a conditional distribution from which we draw particles for each subject. If a model is complex, epsilon is small and there are few new particles on each iteration, we may need more iterations to ensure we get to this space OR we may need more particles to increase the amount of new particles on each iteration - meaning it is quicker to reach the posterior space. Note that increasing the particles and iterations will increase the time taken to run (with increased particles taking longer as this is evaluated for each subject). Ultimately, we should aim for between 10% and 80% new particle rates in the burn-in stage, and by the end of burn-in, we should see stationarity in the parameter estimates. 3.5 Simulating Posterior Predictive Data We can generate posterior predictive data by setting sample = TRUE in our log-likelihood function to generate response times and responses given the posterior parameter estimates for each subject. To do this, we use the gen_pp_data function below, which calls the log-likelihood function embedded in our sampled object. The gen_pp_data function takes four arguments: sampled: is the object/output from the PMwG sampler n: the number of posterior samples ll_func =: the log-likelihood function embedded in the sampled object rbind.data =: bind the rows of each predictive sample into a rectangular array, or leave as a list. gen_pp_data &lt;- function (sampled, n, ll_func = sampled$ll_func, rbind.data = TRUE) { sampled_stage &lt;- length(sampled$samples$stage[sampled$samples$stage == &quot;sample&quot;]) iterations &lt;- round(seq(from = (sampled$samples$idx - sampled_stage), to = sampled$samples$idx, length.out = n)) data &lt;- sampled$data S &lt;- sampled$n_subjects pp_data &lt;- list() for (s in 1:S){ print(paste0(&quot;subject&quot;, s)) for (i in 1:length(iterations)) { print(i) x &lt;- sampled$samples$alpha[, s, iterations[i]] names(x) &lt;- sampled$par_names out &lt;- ll_func(x = x, data = data[data$subject == unique(data$subject)[s], ], sample = TRUE) if (i == 1){ pp_data[[s]] = cbind(pp_iter = i, out) } else { pp_data[[s]] = rbind(pp_data[[s]], cbind(pp_iter = i, out)) } } } if (rbind.data){ tidy_pp_data &lt;- do.call(rbind, pp_data) return(tidy_pp_data) } else { return(pp_data) } } We generate 20 posterior predictive data samples. pp_data_3b &lt;- gen_pp_data(sampled, n = 20) The returned data is a matrix with the same dimensions and names as forstmann – with the addition of pp_iter column. pp_iter is the iteration of posterior sample (in this example i = 1:20) for the corresponding subject. We now have two matrices based on samples from either model. The response (resp) and response time (rt) columns now contain posterior predictive data. In the next section, we will use the posterior predictive data to assess descriptive adequacy. 3.5.1 Assessing Descriptive Adequacy (goodness of fit) Now we will plot the posterior predictive data against the observed data. In the section below we compare observed RTs against predicted RTs, which is common for RT modelling; however, the code could also be modified for different types of data. # Subject x condition Q25, median and Q75 response time + mean accuracy # Forstmann dataset pq3b &lt;- forstmann %&gt;% group_by(condition, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 response time for posterior predictive data pp_pq3b &lt;- pp_data_3b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Combine data with posterior predictive data and add data source pq3b &lt;- bind_rows(cbind(src = rep(&quot;data&quot;, nrow(pq3b)), pq3b), cbind(src = rep(&quot;model&quot;, nrow(pp_pq3b)), pp_pq3b) ) # Mean Q25, median, Q4 and accuracy for data and posterior predictive data av_pq3b &lt;- pq3b %&gt;% group_by(src, condition) %&gt;% summarise_at(vars(Q25:acc), mean) # Variances of posterior samples pp_var3b &lt;- pq3b %&gt;% filter(src != &quot;data&quot;) %&gt;% group_by(condition, pp_iter, src) %&gt;% summarise_at(vars(Q25:acc), mean) # Convert source column to a factor and add labels av_pq3b$src &lt;- factor(av_pq3b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) pp_varsrc3b &lt;- factor(pp_var3b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) # Rename conditions levels(av_pq3b$condition) &lt;- levels(pp_var3b$condition) &lt;- c(&quot;Accuracy&quot;, &quot;Neutral&quot;, &quot;Speed&quot;) # Convert rt to milliseconds and accuracy to percentage av_pq3b$acc &lt;- 100 * av_pq3b$acc pp_var3b$acc &lt;- 100 * pp_var3b$acc av_pq3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * av_pq3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] pp_var3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * pp_var3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] ## Warning: The `legend.title.align` argument of `theme()` is deprecated as of ggplot2 3.5.0. ## ℹ Please use theme(legend.title = element_text(hjust)) instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ##Evaluating different models - single threshold LBA In this section we will evaluate different LBA models. For each model, we will the estimate posterior distribution, generate posterior predictive data, and then compare those against observed data. In this example we assume that each participant’s level of caution does not change with emphasis instructions i.e. the threshold for Accuracy, Neutral, and Speed conditions is the same. We will call this our “single threshold LBA model”. The single threshold LBA model’s log-likelihood has one b parameter, instead of three (see line 18, 31 and 64 below). For each model, we will estimate the posterior distribution, generate posterior predictive data, and then compare these against the empirical data. For brevity, we specify only the ‘fast’ log-likelihood function for the single threshold model. fast_lba_ll1b &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { tmp &lt;- numeric(nrow(data)) data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } if (sample) { for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[&quot;b&quot;] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } } else { vlist = list(&quot;v.1&quot; = numeric(nrow(data)), &quot;v.2&quot; = numeric(nrow(data))) for (c in levels(data$condition)) { for (s in levels(data$stim)) { use &lt;- data$condition == c &amp; data$stim == s vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] if (s == 1) { vlist$v.1[use] = vc vlist$v.2[use] = ve } else { vlist$v.1[use] = ve vlist$v.2[use] = vc } } } out &lt;- dLBA(rt = data$rt, response = data$resp, A = x[&quot;A&quot;], b = x[&quot;b&quot;] + x[&quot;A&quot;], mean_v = vlist, sd_v = c(1, 1), t0 = x[&quot;t0&quot;], distribution = &quot;norm&quot;, silent = TRUE) } if (sample) { return(data) } else { out&lt;-sum(log(pmax(out, 1e-10))) return(out) } } 3.5.2 PMwG framework for a single threshold model The PMwG sampler procedure remains the same for all models. For our three threshold LBA model, we need the updated log-likelihood function from above, an updated parameter vector, start points and priors. # Load the log-likelihood script source(file = &quot;fast_lba_ll1b.R&quot;) # Specify the parameter vector with single threshold (b) parameter pars &lt;- c(&quot;A&quot;,&quot;b&quot;,&quot;vc&quot;,&quot;ve&quot;,&quot;t0&quot;) # Specify a priors list priors &lt;- list( theta_mu = rep(0, length(pars)), theta_sig = diag(rep(1, length(pars))) ) # Setup your sampler object - include your data, parameter vector, # priors and log-likelihood function # Note: Your log-likelihood function must be loaded before this point sampler &lt;- pmwgs( data = forstmann, pars = pars, prior = priors, ll_func = fast_lba_ll1b ) # Start points are not included in this example # Initialise the sampler sampler &lt;- init(sampler) # Burn-in stage burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 500, particles = 1000, epsilon = .5) # Adaptation stage adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;, iter = 10000, particles = 1000, epsilon = .5) # Sample stage sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 1000, particles = 200, epsilon = .5) Note: we keep the priors, start points, number of iterations and particles the same as our three threshold LBA model, so there is no bias for either model. 3.6 Checking Descriptive Adequacy of 1b model. The checking procedure below is the same as the three threshold LBA model, except we use the single threshold LBA model’s sampled object/data. load(&quot;forstmann1b_sampled.RData&quot;) As we did for the three threshold LBA model, we generate 20 posterior predictive data samples. Note: this function is from section 3.5 pp_data_1b &lt;- gen_pp_data(sampled, n = 20) # Subject x condition Q25, median and Q75 response time + mean accuracy # Forstmann dataset pq1b &lt;- forstmann %&gt;% group_by(condition, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 response time for posterior predictive data pp_pq1b &lt;- pp_data_1b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Combine data with posterior predictive data and add data source pq1b &lt;- bind_rows(cbind(src = rep(&quot;data&quot;, nrow(pq1b)), pq1b), cbind(src = rep(&quot;model&quot;, nrow(pp_pq1b)), pp_pq1b) ) # Mean Q25, median, Q4 and accuracy for data and posterior predictive data av_pq1b &lt;- pq1b %&gt;% group_by(src, condition) %&gt;% summarise_at(vars(Q25:acc), mean) # Variances of posterior samples pp_var1b &lt;- pq1b %&gt;% filter(src != &quot;data&quot;) %&gt;% group_by(condition, pp_iter, src) %&gt;% summarise_at(vars(Q25:acc), mean) # Convert source column to a factor and add labels av_pq1b$src &lt;- factor(av_pq1b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) pp_varsrc1b &lt;- factor(pp_var1b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) # Rename conditions levels(av_pq1b$condition) &lt;- levels(pp_var1b$condition) &lt;- c(&quot;Accuracy&quot;, &quot;Neutral&quot;, &quot;Speed&quot;) # Convert rt to milliseconds and accuracy to percentage av_pq1b$acc &lt;- 100 * av_pq1b$acc pp_var1b$acc &lt;- 100 * pp_var1b$acc av_pq1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * av_pq1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] pp_var1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * pp_var1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] 3.7 Model Comparison In this section, we are going to compare the three threshold model and the single threshold model for the Forstmann et al. (2008) data to determine which model best represents the data. We begin by using a graphical method i.e. plotting the modelled data against the observed data. 3.7.1 Assessing Descriptive Adequacy Graphically We can assess descriptive adequacy graphically by plotting the observed data, three threshold model and single threshold model data on a single plot. # Subject x condition Q25, median and Q75 response time + mean accuracy # Forstmann data pq &lt;- forstmann %&gt;% group_by(condition, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 response time for posterior predictive data #3b model pp_pq3b &lt;- pp_data_3b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 response time for posterior predictive data #1b model pp_pq1b &lt;- pp_data_1b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Combine data with posterior predictive data for 1b and 3b models + add data source pqall &lt;- bind_rows(cbind(src = rep(&quot;data&quot;, nrow(pq)), pq), cbind(src = rep(&quot;3b&quot;, nrow(pp_pq3b)), pp_pq3b), cbind(src = rep(&quot;1b&quot;, nrow(pp_pq1b)), pp_pq1b) ) # Mean Q25, median, Q4 and accuracy for data and posterior predictive data av_pq &lt;- pqall %&gt;% group_by(src, condition) %&gt;% summarise_at(vars(Q25:acc), mean) # Variances of posterior samples pp_var &lt;- pqall %&gt;% filter(src != &quot;data&quot;) %&gt;% group_by(condition, pp_iter, src) %&gt;% summarise_at(vars(Q25:acc), mean) # Convert source column to a factor and add labels av_pq$src &lt;- factor(av_pq$src, levels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;), labels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;) ) pp_varsrc &lt;- factor(pp_var$src, levels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;), labels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;) ) # Rename conditions levels(av_pq$condition) &lt;- levels(pp_var$condition) &lt;- c(&quot;Accuracy&quot;, &quot;Neutral&quot;, &quot;Speed&quot;) # Convert rt to milliseconds and accuracy to percentage av_pq$acc &lt;- 100 * av_pq$acc pp_var$acc &lt;- 100 * pp_var$acc av_pq[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * av_pq[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] pp_var[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * pp_var[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] The plots show that values in the three threshold model (red circles) better describe the data than the single threshold model (blue circles). 3.7.2 Model comparison via DIC Another method of model comparison is using an information criterion such as the deviance information criterion (DIC). Here we provide a function for calculating DIC. Note: we recommend using marginal likelihood (Bayes factors) instead of DIC for model selection. For more information, see this paper on estimating the Marginal Likelihood via importance sampling. pmwg_DIC &lt;- function(sampled, pD = FALSE){ # Identify number of subjects nsubj &lt;- length(unique(sampled$data$subject)) # Mean log-likelihood of the overall (sampled-stage) model, for each subject mean_ll &lt;- apply(sampled$samples$subj_ll[, sampled$samples$stage == &quot;sample&quot;], 1, mean) # Mean of each parameter across iterations. # Keep dimensions for parameters and subjects mean_pars &lt;- t(apply(sampled$samples$alpha[,, sampled$samples$stage == &quot;sample&quot;], 1:2, mean)) # Name &#39;mean_pars&#39; so it can be used by the log_like function colnames(mean_pars) &lt;- sampled$par_names # log-likelihood for each subject using their mean parameter vector mean_pars_ll &lt;- numeric(ncol(mean_pars)) data &lt;- transform(sampled$data, subject = match(subject, unique(subject))) for (j in 1:nsubj) { mean_pars_ll[j] &lt;- sampled$ll_func(mean_pars[j, ], data = data[data$subject == j,], sample = FALSE) } # Effective number of parameters pD &lt;- sum(-2 * mean_ll + 2 * mean_pars_ll) # Deviance Information Criterion DIC &lt;- sum(-4 * mean_ll + 2 * mean_pars_ll) if (pD){ return(c(&quot;DIC &quot; = DIC, &quot; Effective parameters&quot; = pD)) }else{ return(DIC) } } We calculate the DIC value for each model by passing the sampled object into the pmwgDIC function. load(&quot;forstmann3b_sampled.RData&quot;) sampled3b &lt;- sampled pmwg_DIC(sampled = sampled3b) DIC Effective parameters -15381.53339 91.76401 load(&quot;forstmann1b_sampled.RData&quot;) sampled1b &lt;- sampled pmwg_DIC(sampled = sampled1b) DIC Effective parameters -10778.94733 44.84388 The three threshold model has a lower DIC value than the single threshold model, indicating a better explanation of the data. Based on the graphical evidence and the DIC, we can be confident that the three threshold model (i.e the model where threshold is varied with speed and accuracy instructions) is a better fit to our data than the single threshold model. 3.8 Checking the LBA log-likelihood function Here we show two ways to test your log-likelihood function. This method is not fail-safe, however, it allows you to establish whether your function operates as intended. We will use the three threshold LBA model log-likelihood function in the following examples. 3.8.1 Test one: Do changes in parameter values cause changes in the returned log-likelihood? We will test our log-likelihood functions by passing a parameter vector (x) of plausible, arbitrary values, followed by a second parameter vector with different, plausible, arbitrary values. If the log-likelihood functions are functioning correctly, we should see the log-likelihood value change with the change in parameter values. We may also wish to check that unrealistic parameter values are effectively dealt with (for example, t0 should not be able to be greater than any RTs). Let’s compare the output from the trialwise log-likelihood function in section 3.3 and the fast log-likelihood function in section 3.3.3 : x &lt;- log(c(A = 2, b.1 = 2, b.2 = 1, b.3 = .5, vc = 4, ve = 2, t0 = .18 ) ) tw_lba_ll(x, forstmann, sample = FALSE) ## [1] -44505.89 fast_lba_ll3b(x, forstmann, sample = FALSE) ## [1] -44505.89 Great – the log-likelihoods are the same. Now let’s change the parameter values, in particular, the three threshold parameters, and see if the log-likelihood changes. x &lt;- log(c(A = 2, b.1 = 1, b.2 = 3, b.3 = 3, vc = 4, ve = 2, t0 = .18 ) ) tw_lba_ll(x, forstmann, sample = FALSE) ## [1] -202667.3 fast_lba_ll3b(x, forstmann, sample = FALSE) ## [1] -202667.3 These log-likelihoods are lower than the previous two, so the parameter (x) values are less accurate given the data. Notice how we can also use this process to check the fast, efficient log-likelihood function (fast_lba_ll3b) against the trialwise function (tw_lba_ll). Given the same data and parameter values - they should give the same output, as we see above. Note: We only changed some named (x) parameter values. You could perform a thorough check by changing one parameter value at a time and see if there is a change in the log-likelihood. This is a good first check to see if our log-likelihood is behaving as it should; change in log-likelihood value with a change in parameter values. However, our log-likelihood functions may contain inconsistencies, which leads us onto our second check. 3.8.2 Testing whether data generating parameter values have the highest likelihood A more comprehensive way to test your log-likelihood function is to generate ‘synthetic’ data for which you know the data generating values. This is a form of ‘parameter recovery’, where you run the sampler on generated data and plot the log-likelihood change as x values move away from your data generating parameter x-values. If the data generating x-values have the highest log-likelihood, then it is likely that your function is working as intended. There are two steps to this method: Assign values to parameters of the model - these are known as the generating values. Note: ensure these are sensible values - for example in the LBA, we want the drift rate for the mismatch/error accumulator (ve) to be smaller than the match/correct accumulator (vc) and the non-decision time parameter t0 should not be too large. x &lt;- log(c(A = 2, b.1 = 1, b.2 = 3, b.3 = 3, vc = 4, ve = 2, t0 = .18 ) ) Produce profile plots to compare the log-likelihood of the x parameter values against nearby values in the ‘synthetic’ data. You can write your own function for this, or use the function below. Our plotting function takes two arguments: sampler is the initiated sampler object. See section 3.4.2 above. generating_values = Null create ‘generating values’ based on theta_mu from sampler object or use values output from the log-likelihood function. profilePlot &lt;- function(sampler, generating_values = NULL){ if(is.null(generating_values)){ # Create generating values based on theta_mu generating_values &lt;- sampler$samples$theta_mu names(generating_values) &lt;- sampler$par_names } else{ names(generating_values) &lt;- sampler$par_names } #Create test data set. We use a tenth of the total data for speed test &lt;- sampler$ll_func(x = generating_values, data = sampler$data[c(1:(nrow(sampler$data) / 3)),], sample = TRUE) # n_values = number of values to test and plot. n_values &lt;- 9 tmp &lt;- array(dim = c(n_values, sampler$n_pars)) # Scale for increment. Can be made smaller or larger. # Here we use theta_mu - .2 to theta_mu + .2, with n_values length increment &lt;- seq(from = -.2, to = .2, length.out = n_values) for (i in 1:sampler$n_pars){ for (j in 1:n_values){ # Use all generating values except the current parameter being tested test_values &lt;- generating_values # Change the current parameter by adding the increment test_values[i] &lt;- generating_values[i] + increment[j] # Test the likelihood given these new values and the test data tmp[j, i] &lt;- sampler$ll_func(x = test_values, data = test, sample = FALSE) } } # Prepare output for plotting colnames(tmp) &lt;- sampler$par_names tmp &lt;- as.data.frame(tmp) tmp &lt;- tidyr::pivot_longer(tmp, everything(), names_to = &quot;pars&quot;, values_to = &quot;likelihood&quot;) tmp$increment &lt;- rep(increment, each = sampler$n_pars) # Plot values for each parameter ggplot2::ggplot(tmp, aes(x = increment, y = likelihood) ) + geom_point() + geom_line() + facet_wrap(~pars, scales = &quot;free&quot;) + theme_bw() + theme(axis.title.y = element_text(size = 14, face = &#39;bold&#39;), axis.text.y = element_text(size=14), axis.title.x = element_text(size = 14, face = &#39;bold&#39;), axis.text.x = element_text(size=14) ) + labs(x = &quot;Increment&quot;, y = &quot;Likelihood&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;, alpha = 0.3) } profilePlot(sampler = sampler, generating_values = x) The red line indicates the true generating parameter value (i.e. the data generating values for the parameters) have the best (highest) likelihoods. Note: The plots above do not tell you if your log-likelihood is correct in the sense that you have written a log-likelihood for the model you actually want to test. The plots allow you to determine if your function is functioning as intended. Below is output from a function with a potential error. We can see that the red lines indicating the generating parameter values (i.e. the data generating values for the parameters) do not have the best (highest) likelihoods. Plots with flat lines for parameter values would also be indicative of an error somewhere within your log-likelihood code. References Brown, Scott, and Andrew Heathcote. 2008. “The Simplest Complete Model of Choice Response Time: Linear Ballistic Accumulation.” Cognitive Psychology 57 (3): 153–78. Forstmann, Birte U, Gilles Dutilh, Scott Brown, Jane Neumann, D Yves Von Cramon, K Richard Ridderinkhof, and Eric-Jan Wagenmakers. 2008. “Striatum and Pre-SMA Facilitate Decision-Making Under Time Pressure.” Proceedings of the National Academy of Sciences 105 (45): 17538–42. The parameters you list in the pars vector must match those included in your log-likelihood function i.e. they must have the same names and the same number of parameters. Refer to the troubleshooting section for more detail.↩︎ "],["pmwg-sampler-with-the-linear-ballistic-accumulator-and-a-complex-experiment-design.html", "Chapter 4 PMwG sampler with the Linear Ballistic Accumulator and a complex experiment design 4.1 The LBA log-likelihood function for the Wagenmakers data set 4.2 Fast LBA Log-likelihood function 4.3 PMwG Framework 4.4 Simulating Posterior Predictive Data 4.5 Model Comparison via DIC", " Chapter 4 PMwG sampler with the Linear Ballistic Accumulator and a complex experiment design In chapter 2 we demonstrated how the PMwG sampler can be used to model a lexical decision task in a signal detection framework. However, the SDT framework does not allow us to consider response time (RT) and the join distribution of RT and accuracy. In this example we will expand on what was covered in chapter 2, by fitting the LBA - a more complex model - to the Wagenmakers 2008 data. A description of the Wagenmakers experiment and data is covered in chapter 2. This experiment is more complicated than the Forstmann example in chapter 3, and the LBA is also more complicated than the SDT model. As a result, the log-likelihood function for this example will be more complex, however, you’ll notice that each step closely follows those taken in previous chapters with simpler data sets and simpler models. 4.1 The LBA log-likelihood function for the Wagenmakers data set 4.1.1 What is a log-likelihood function? If you’re unsure what a log-likelihood function is and/or does, see our explanation here. 4.1.2 Writing the LBA log-likelihood for the Wagenmakers data set As we have shown in the previous two examples, we include a computationally-slow and easy to follow log-likelihood function. The log-likelihood function steps through the data line by line (i.e. trial by trial) and gives a likelihood value for each line given x parameters. As mentioned in previous chapters, we encourage those who have experience writing likelihood functions to write a computationally efficient function or use our fast LBA log-likelihood function. For those new to modelling, the trialwise function directly below is easier to follow, debug and is less likely to result in errors. The down side is that the slow/trialwise log-likelihood function is approximately five times slower to run than the fast log-likelihood function. The structure of our log-likelihood function follows those in the preceding chapters, so we will focus on the parts that differ i.e. the experiment design and hypothesis about which parameters are being influenced by the experimental manipulations. See chapter 3 for an outline of the LBA parameters. Let’s begin by loading rtdists package… library(rtdists) …and now a complete trialwise (slow) log-likelihood function. tw_lba_ll &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b.w = x[paste0(&quot;b.&quot;, data$cond[i], &quot;.W&quot;)] + A b.nw = x[paste0(&quot;b.&quot;, data$cond[i], &quot;.NW&quot;)] + A bs = list(b.nw, b.w) v.w = x[paste0(&quot;v.&quot;, data$stim[i], &quot;.W&quot;)] v.nw = x[paste0(&quot;v.&quot;, data$stim[i], &quot;.NW&quot;)] vs = c(v.nw, v.w) t0 = x[&quot;t0&quot;] s = c(1, 1) if (sample) { tmp &lt;- rLBA(n = 1, A = A, b = bs, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } else { out[i] &lt;- dLBA(rt = data$rt[i], response = data$resp[i], A = A, b = bs, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) } } if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } } Note: If you’d like to run through this example, it is best to copy the tw_lba_ll function from the code block above rather than copying from the separate code chunks below where curly braces have been removed. We begin from the for loop on line 14. For for each row in the dataset, we assign the values (x) of each parameter in our model so that any conditional parameters (for example b in our model) are correctly assigned. For the Wagenmakers data set, we want to calculate the density function for a model that has a threshold (b) parameter for each of the conditions (cond = w: 75% words &amp; 25% non-words, nw: 75% non-words &amp; 25% words). We also want threshold to vary for response (resp) type (i.e., the accumulator for a word (W) response, and the accumulator for the non-word (NW). So, on lines 16 and 17, we paste together the \"b.\" threshold parameter, the condition (cond = w or nw) and the response \".W\" or \".NW\" and add the start point parameter A. The start point parameter is added to the threshold values to ensure that threshold is greater than the start point value. We hypothesised that drift rate v would vary with word frequency (stim = hf,lf,vlf,nw), so on lines 19 and 20 we allow drift rate for response word (v.w) to vary with the levels of word frequency, by pasting the \"v.\" with stim and with the accumulator for each response (\".W\" or \".NW\"). You’ll notice that in this example, we no longer have a drift rate for the correct response (vc) or incorrect response (ve), instead, we have a drift rate for responding word (v.w) and non-word (v.nw). This is a different way (and our preferred way) of coding drift rate. On line 21 we have ordered the vs vector with v.nw first and v.w second. for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b.w = x[paste0(&quot;b.&quot;, data$cond[i], &quot;.W&quot;)] + A b.nw = x[paste0(&quot;b.&quot;, data$cond[i], &quot;.NW&quot;)] + A bs = list(b.nw, b.w) v.w = x[paste0(&quot;v.&quot;, data$stim[i], &quot;.W&quot;)] v.nw = x[paste0(&quot;v.&quot;, data$stim[i], &quot;.NW&quot;)] vs = c(v.nw, v.w) t0 = x[&quot;t0&quot;] s = c(1, 1) It is important to check the order of your levels for the response factor to ensure you correctly order your v.nw and v.w accumulators in the vs vector. levels(data$resp) ## [1] &quot;1&quot; &quot;2&quot; If we look at our levels for response, we see the order is NW and W respectively. This means that NW is coded as 1 and W is coded as 2. Therefore, on line 36, if data$resp[i] = N.W then the level is 1, and the first accumulator in position 1 will be chosen. If data$resp[i] = W this tells the sampler that the accumulator in position 2 will be chosen. In other words, the parameters will map correctly because v.nw is in position 1 and v.w in position 2 in our vs vector. The remaining lines of the log-likelihood function are identical to the function used in the Forstmann example in chapter 3, except the threshold parameter bs (line 29 and 44) is a list with two elements (b = bs) 4.2 Fast LBA Log-likelihood function As we mentioned in section 3.3.3, the data is large and the dLBA function takes some time to run, so the log-likelihood code above is computationally inefficient. There are several ways to improve the log-likelihood’s performance; in our example below, we reduce the number of calls to dLBA to one call. We do this by passing a list of dLBA parameter values for the length of the data. Note: When generating posterior predictive data, the rLBA function is still executed for each row of data; however, it is only executed several times, so computational efficiency is not compromised. fast_lba_ll &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } b.w = b.nw = v.w = v.nw = out = numeric(nrow(data)) for (p in unique(data$prop)) { use &lt;- data$cond == p b.w[use] = x[paste0(&quot;b.&quot;, p, &quot;.W&quot;)] + x[&quot;A&quot;] b.nw[use] = x[paste0(&quot;b.&quot;, p, &quot;.NW&quot;)] + x[&quot;A&quot;] } for (f in unique(data$freq)) { use &lt;- data$stim == f v.w[use] = x[paste0(&quot;v.&quot;, f, &quot;.W&quot;)] v.nw[use] = x[paste0(&quot;v.&quot;, f, &quot;.NW&quot;)] } bs = list(b.nw,b.w) vs = list(v.nw,v.w) if (!sample){ out &lt;- dLBA(rt = data$rt, resp = data$resp, A = x[&quot;A&quot;], b = bs, t0 = x[&quot;t0&quot;], mean_v = vs, sd_v = c(1, 1), distribution = &quot;norm&quot;, silent = TRUE) out &lt;- sum(log(pmax(out,1e-10))) return(out) } else { data$rt &lt;- NA data$resp &lt;- NA for (i in 1:nrow(data)){ tmp&lt;- rLBA(n = 1, A = x[&quot;A&quot;], b = list(b.nw[i], b.w[i]), t0 = x[&quot;t0&quot;], mean_v = list(v.nw[i], v.w[i]), sd_v = c(1, 1), distribution = &quot;norm&quot;, silent = TRUE) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } return(data) } } You should improve your log-likelihood’s performance as you see fit. When you’re confident that your log-likelihood code functions correctly, we suggest saving it as a separate script so it can be sourced and loaded when running the sampler. If you’re learning how to write log-likelihood functions, take a look at our troubleshooting section for tips. In our experience, a very large proportion of problems with sampling and inference are caused by inadequate checking and care in the likelihood function. 4.3 PMwG Framework Now that we have a log-likelihood function, we can set up the PMwG sampler. To run the sampler, we simply follow the procedure from chapter 2 and chapter 3; we need to set up a vector of model parameter names, create a priors object, source our LBA log-likelihood script and then create our sampler object. First, load the PMwG library and data… library(pmwg) load(&quot;dataObjects/wagenmakers2008.RData&quot;) Let’s create a vector of model parameter names. Remember that this must match the names and number of parameters you include in your log-likelihood function. You can name this vector as you wish; however, in our example, we name it pars. For the Wagenmakers dataset, we require four threshold parameters because we assume that both word proportion (cond) and response (resp) have an effect on level of caution (i.e. a threshold parameter for each combination of word proportion and response). We also need a drift rate for each cell of our design i.e. eight drift rate parameters; one for each level of word frequency (hf,lf,vlf,nw) for the Word response accumulator .W, and the non-word response accumulator .NW). We’ve made a decision to set the sv to 1 to satisfy the scaling properties of the model. As such, we haven’t included the sv parameter in the pars vector - it is found in the LBA’s log-likelihood function (see below). pars &lt;- c(&quot;A&quot;, &quot;b.nw.NW&quot;, &quot;b.nw.W&quot;, &quot;b.w.NW&quot;, &quot;b.w.W&quot;, &quot;v.hf.NW&quot;, &quot;v.hf.W&quot;, &quot;v.lf.NW&quot;, &quot;v.lf.W&quot;, &quot;v.vlf.NW&quot;, &quot;v.vlf.W&quot;, &quot;v.nw.NW&quot;, &quot;v.nw.W&quot;, &quot;t0&quot;) For the mean of the distribution for random effects (theta_mu), we assume a multivariate normal prior. The user can specify the mean and variance of this prior distribution using the object priors, which has elements theta_mu_mean and theta_mu_var. A typical setting for LBA models is to set theta_mu_mean to be a zero vector and to set theta_mu_var to be a multiple of the identity matrix, e.g. with 9 on the diagonal (representing a standard deviation of 3 for the subject-level means in the prior). We create our priors object; a list that contains two components: theta_mu_mean a vector containing the prior for model parameter means theta_mu_var the prior covariance matrix for model parameters. priors &lt;- list(theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(1, length(pars))) ) Now source and load your log-likelihood script before you create the sampler object. source(file = &quot;fast_lba_ll.R&quot;) Next we specify the PMwG sampler object. The pmwgs function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps. data = your data - a data frame (e.g.wagenmakers) with a column for participants called subject pars = the model parameters to be used (e.g.pars) prior = the priors to be used (e.g.priors) ll_func = name of log-likelihood function to be used (e.g.fast_lba_ll) sampler &lt;- pmwgs(data = data, pars = pars, prior = priors, ll_func = fast_lba_ll ) 4.3.1 Running the sampler Setup is now complete and we can run the sampler. First, we use the init function to generate initial start points for the random effects and store them in the sampler object. If you wanted you could include start points for your chain. We will not specify start points as we did in the Forstmann three threshold model. This is why have not included a start_mu and a start_sig. Instead, the start points will be drawn from the prior. Note: The init stage can take some time to run because it uses a large number of particles. sampler &lt;- init(sampler) To run the sampler, we use the run_stage function. To execute the run_stage function, you must provide values for two arguments: pmwgs = the sampler object including parameters that were created by the init function above. stage = the sampling stage (In order; \"burn\", \"adapt\" or \"sample\"). The following arguments listed below are optional: iter = is the number of iterations for the sampling stage. For burn-in, it is important to have enough iterations that the chains converge on the posterior distribution. Default = 1000. particles = is the number of proposals (particles) generated for each random effect, on each iteration. Default = 1000, but set smaller or larger in order to target a reasonable acceptance rate (i.e. 10-60%). display_progress = display a progress bar during sampling epsilon = is a value greater than 0 which scales the variance of the proposal distribution. Smaller values (i.e. narrower proposal distributions) can lead to higher acceptance rates, but slower coverage of the posterior. Smaller values are especially useful when the number of random effects is large (e.g. &gt;10). The default is adaptively chosen based on the number of parameters. n_cores = the number of cores on a machine you wish to use to run the sampler. This allows sampling to be run across cores (parallelising for subjects). Default = 1. Note: Setting n_cores greater than 1 is only permitted on Linux and Mac OS X machines. The first sampling stage is burn-in \"burn\". The burn-in stage allows time for the sampler to move from the (arbitrary) start points that were provided by the user to the mode of the posterior distribution. This can be checked by examining the chains for stationarity. We take the sampler object created in the init function above, set the stage argument to \"burn\" and assign the outcome to an object called burned. Iterations for burn-in (in this example iter = 1000) are set by choosing a sufficient number of iterations that will get the sampler to the posterior space Note: You should visually check the chains for convergence/stationarity after burn-in. burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 1000, particles = 100 ) Next is the adaptation stage \"adapt\". The adaptation stage draws samples using a simple, but relatively inefficient proposal distribution (the same proposal distribution as the \"burn\"stage). Enough samples are drawn to allow the algorithm to estimate a much more sophisticated and efficient proposal distribution, using conditional normal distributions. We take the burned object created in the previous stage and set iterations iter = to a high number (e.g. 10000), as it should exit before reaching this point. If it doesn’t, there is likely an issue with acceptance rates, the likelihood function or limited data to operate on (i.e. few trials in some conditions). Here, we have saved the outcome of the adaptation stage to an object called adapted. adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;, iter = 5000, particles = 100 ) The final stage is the sampling stage \"sample\". The sampling stage uses the sophisticated and adaptive conditional normal proposal distributions. This allows for very efficient sampling, using far fewer particles. Samples from this stage are taken from the ‘posterior distribution’ and stored in the sampled object. sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 10000, particles = 100 ) The sampled object includes all samples from the \"sample\" stage above and the following elements: data : the data (data frame) you included in your analysis par_names: parameter names n_pars: number of parameters n_subjects: number of subjects subjects: subject IDs (1:n) prior: list of the prior used ll_func: the likelihood function specified samples: alpha: three dimensional array of random effects draws (dim = parameters x subjects x samples) theta_mu: two dimensional array of parameter draws (dim = parameters x samples) theta_sig: three dimensional array of covariance matrix draws (dim = covariance x samples) stage: specifies the stage the sample is from (length = samples) subj_ll: likelihood value for each subject for each iteration (dim = subject x samples) a_half: the parameter used in calculating the inverse Wishart (dim = parameters x samples) idx: total number of samples last_theta_sig_inv: the inverse of the last sample for theta_sig (the variance-covariance matrix). You should save your sampled object at this point. save(sampled, file = &quot;wagSamp.RData&quot;) For help with determining an appropriate number of iterations and particles required for your model estimation, see estimation settings for the PMwG sampler in chapter 3. 4.4 Simulating Posterior Predictive Data We can generate posterior predictive data by setting sample = TRUE in our log-likelihood function to generate response times and responses given the posterior parameter estimates for each subject. To do this, we use the gen_pp_data function below, which calls the log-likelihood function embedded in our sampled object. The gen_pp_data function takes four arguments: sampled: is the object/output from the PMwG sampler n: the number of posterior samples ll_func =: the log-likelihood function embedded in the sampled object rbind.data =: bind the rows of each predictive sample into a rectangular array, or leave as a list. gen_pp_data &lt;- function (sampled, n, ll_func = sampled$ll_func, rbind.data = TRUE) { sampled_stage &lt;- length(sampled$samples$stage[sampled$samples$stage == &quot;sample&quot;]) iterations &lt;- round(seq(from = (sampled$samples$idx - sampled_stage), to = sampled$samples$idx, length.out = n)) data &lt;- sampled$data S &lt;- sampled$n_subjects pp_data &lt;- list() for (s in 1:S){ print(paste0(&quot;subject&quot;, s)) for (i in 1:length(iterations)) { print(i) x &lt;- sampled$samples$alpha[, s, iterations[i]] names(x) &lt;- sampled$par_names out &lt;- ll_func(x = x, data = data[data$subject == unique(data$subject)[s], ], sample = TRUE) if (i == 1){ pp_data[[s]] = cbind(pp_iter = i, out) } else { pp_data[[s]] = rbind(pp_data[[s]], cbind(pp_iter = i, out)) } } } if (rbind.data){ tidy_pp_data &lt;- do.call(rbind, pp_data) return(tidy_pp_data) } else { return(pp_data) } } We generate 20 posterior predictive data samples. ppdataWag &lt;- gen_pp_data(sampled, n = 20) The returned data is a matrix with the same dimensions and names as wagenmakers – with the addition of pp_iter column. pp_iter is the iteration of the posterior sample (in this example i = 1:20) for the corresponding subject. We now have two matrices based on samples from either model. The response (resp) and response time (rt) columns now contain posterior predictive data. In the next section, we will use the posterior predictive data to assess descriptive adequacy. 4.4.1 Assessing Descriptive Adequacy (goodness of fit) Now we will plot the posterior predictive data against the real data. In the section below we compare observed RTs against predicted RTs, which is common for RT modelling; however, the code could also be modified for different types of data. # Recode condition column in order to calculate accuracy # Data wagenmakers$condition &lt;- recode(wagenmakers$condition, w = &quot;W&quot;, nw =&quot;NW&quot; ) wagenmakers$correct &lt;- wagenmakers$correct - 1 # Posterior predictive data ppdataWag$condition &lt;- recode(ppdataWag$condition, w = &quot;W&quot;, nw =&quot;NW&quot; ) ppdataWag$binary_stim &lt;- ifelse(ppdataWag$stim ==&quot;nw&quot;, 1, 2) ppdataWag$correct &lt;- ifelse(ppdataWag$binary_stim == ppdataWag$response, 1, 0) # Subject x condition Q25, median and Q75 response time + mean accuracy # Wagenmakers dataset pqWag &lt;- wagenmakers %&gt;% group_by(subject, condition, stim) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(correct), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 response time for posterior predictive data pp_pqWag &lt;- ppdataWag %&gt;% group_by(condition, pp_iter, subject, stim) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(correct), .groups = &quot;keep&quot; ) # Combine data with posterior predictive data and add data source pqWag &lt;- bind_rows(cbind(src = rep(&quot;data&quot;, nrow(pqWag)), pqWag), cbind(src = rep(&quot;model&quot;, nrow(pp_pqWag)), pp_pqWag) ) # Mean Q25, median, Q4 and accuracy for data and posterior predictive data av_pqWag &lt;- pqWag %&gt;% group_by(src, condition, stim) %&gt;% summarise_at(vars(Q25:acc), mean) # Variances of posterior samples pp_varWag &lt;- pqWag %&gt;% filter(src != &quot;data&quot;) %&gt;% group_by(condition, pp_iter, src, stim) %&gt;% summarise_at(vars(Q25:acc), mean) # Convert source column to a factor and add labels av_pqWag$src &lt;- factor(av_pqWag$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) pp_varsrcWag &lt;- factor(pp_varWag$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) # Rename conditions levels(av_pqWag$condition) &lt;- levels(pp_varWag$condition) &lt;- c(&quot;75% \\nNon-Word \\nstimuli&quot;,&quot;75% \\nWord stimuli&quot;) # Change plot stimuli labels # Convert rt to milliseconds and accuracy to percentage av_pqWag$acc &lt;- 100 * av_pqWag$acc pp_varWag$acc &lt;- 100 * pp_varWag$acc av_pqWag[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * av_pqWag[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] pp_varWag[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * pp_varWag[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] 4.5 Model Comparison via DIC Deviance information criterion (DIC) can be used as a metric to compare two models, and assess which model fits the data best, with penalties applied to model complexity. Note: we recommend using marginal likelihood (Bayes factors) instead of DIC for model selection. For more information, see this paper on estimating the Marginal Likelihood via importance sampling. In this chapter we have covered one LBA model, however; we have included a DIC function and DIC value for completeness. To see an example where DIC is used as a model selection technique, see chapter 3. pmwg_DIC &lt;- function(sampled, pD = FALSE){ # Identify number of subjects nsubj &lt;- length(unique(sampled$data$subject)) # Mean log-likelihood of the overall (sampled-stage) model, for each subject mean_ll &lt;- apply(sampled$samples$subj_ll[, sampled$samples$stage == &quot;sample&quot;], 1, mean) # Mean of each parameter across iterations. # Keep dimensions for parameters and subjects mean_pars &lt;- t(apply(sampled$samples$alpha[,, sampled$samples$stage == &quot;sample&quot;], 1:2, mean)) # Name &#39;mean_pars&#39; so it can be used by the log_like function colnames(mean_pars) &lt;- sampled$par_names # log-likelihood for each subject using their mean parameter vector mean_pars_ll &lt;- numeric(ncol(mean_pars)) data &lt;- transform(sampled$data, subject = match(subject, unique(subject))) for (j in 1:nsubj) { mean_pars_ll[j] &lt;- sampled$ll_func(mean_pars[j, ], data = data[data$subject == j,], sample = FALSE) } # Effective number of parameters pD &lt;- sum(-2 * mean_ll + 2 * mean_pars_ll) # Deviance Information Criterion DIC &lt;- sum(-4 * mean_ll + 2 * mean_pars_ll) if (pD){ return(c(&quot;DIC &quot; = DIC, &quot; Effective parameters&quot; = pD)) }else{ return(DIC) } } We calculate a DIC value for a model by passing the sampled object into the pmwgDIC function. # Load Wagenmakers sampled object and assign to variable &quot;sampledWag&quot; load(&quot;wagLBAsamp.RData&quot;) sampledWag &lt;- sampled pmwg_DIC(sampled = sampledWag) DIC Effective parameters -20085.6336 224.2085 "],["estimating-the-marginal-likelihood-via-importance-sampling-is2.html", "Chapter 5 Estimating the Marginal Likelihood via Importance Sampling (IS2) 5.1 Using IS2 with the Forstmann dataset", " Chapter 5 Estimating the Marginal Likelihood via Importance Sampling (IS2) In chapter 3, we outlined two model comparison methods (i.e. graphical and information criterion) to determine the “best fitting” models for the Forstmann dataset. However, the current gold standard for model selection/comparison is to use marginal likelihood with Bayes factors. In this chapter we will illustrate how you can estimate marginal likelihood via importance sampling squared (IS2). The IS2 method is a robust estimation method that accounts for model flexibility and provides unbiased estimates of the marginal likelihood. Marginal likelihood estimates allow you to assess the fit of a model and the model’s flexibility by integrating the likelihood across the prior predictive space of a model. In hierarchical models, obtaining the marginal likelihood is difficult, as the likelihood function is the density of the data with the random effects integrated out when viewed as a function of the group-level parameters; an integral which is often unavailable (computationally or as it is intractable). Despite this integral being intractable, IS2 allows a method of estimating the marginal likelihood when the likelihood is intractable but can be estimated without bias. The method works by first sampling from the posterior via a sampling scheme such as MCMC (or here, PMwG). These draws are then used to create the importance distribution for the fixed parameters. This importance distribution is constructed by fitting a mixture of normal or Student t distributions to these MCMC samples. We then construct conditional proposal parameters - called particles - for each subject. The marginal likelihood is then estimated in an unbiased manner which is combined with the importance distribution. From this method, the importance sampling procedure is in itself an importance sampling procedure which can be used to estimate the likelihood. 5.1 Using IS2 with the Forstmann dataset Here we will demonstrate how to use the IS2 algorithm to compare models from the Forstmann example in Chapter 3. In the example shown here, we use samples taken from the two Forstmann (2008) models shown in chapter 3. We use samples from the PMwG posterior sampling stage (the sampled object). IS2 is robust enough to be able to use samples from any stage of PMwG; however, we recommend sampling from the posterior for lower variance. In this example we run IS2 to compare the four models in an unbiased manner. The IS2 paper uses the same method, and so this chapter provides a walk-through of that example. As we are using samples from the PMwG algorithm, the prior_dist function is specifically coded for PMwG priors. To run other models, the prior_dist function needs to be updated (more on this later maybe). 5.1.1 Load packages and samples First we load the required packages and set up the environment, including the number of CPUs to use and the sampled object. # Load and install required packages rm(list=ls()) pkgs &lt;- c(&quot;mvtnorm&quot;, &quot;MCMCpack&quot;, &quot;rtdists&quot;, &quot;invgamma&quot;, &quot;mixtools&quot;, &quot;condMVNorm&quot;, &quot;parallel&quot;) for (pkg in pkgs){ if (!require(pkg, character.only = T)){ install.packages(pkg) library(pkg) } } load(&quot;forstmannM1_sampled.Rdata&quot;) cpus = 32 5.1.2 Set up variables Next we set up the variables to be used by the algorithm. With PMwG these can remain as specified here. Essentially the algorithm needs the number of subjects, random effects, iterations of samples, number of required IS2 samples, number of IS2 particles and the parameter names. Here, for convenience we use 1000 samples and 250 particles. It is often more reliable to run a larger number of samples and particles, however, this decreases efficiency. We recommend reading blog post for more information. We also recommend running the IS2 algorithm for several iterations and combining the IS2 samples output to achieve more stable estimates. # Set up variables for IS2 # Get some properties of the sampled object # Number of random effects n_randeffect &lt;- sampled$n_pars # Number of subjects n_subjects &lt;- sampled$n_subjects # Number of sample iterations n_iter &lt;- length(sampled$samples$stage[sampled$samples$stage==&quot;sample&quot;]) # Length of the full transformed random effect vector and/or parameter vector length_draws &lt;- sampled$samples$idx # Number of importance samples IS_samples &lt;- 1000 # Number of particles n_particles &lt;- 250 # Parameter values pars &lt;- sampled$pars 5.1.3 Store the samples In this next step, we store the outputs from PMwG to be used in the IS2 algorithm. This leads to us creating X, an array of all parameters, random effects, off diagonal covariances and a-half values used in the PMwG sampling by the length of posterior samples. # Store the random effects from the sampled stage of PMwG object alpha &lt;- sampled$samples$alpha[,, sampled$samples$stage == &quot;sample&quot;] # Store theta mu from the sampled stage of PMwG object theta &lt;- sampled$samples$theta_mu[, sampled$samples$stage == &quot;sample&quot;] # Store the Cholesky transformed sigma from the sampled stage of PMwG object sig &lt;- sampled$samples$theta_sig[,, sampled$samples$stage == &quot;sample&quot;] # The a-half is used when calculating the Huang and Wand (2013) prior. # The &#39;a&#39; is a random sample from the inverse gamma which weights the inverse Wishart. The mix of inverse Wisharts is the prior on the correlation matrix a_half &lt;- log(sampled$samples$a_half[, sampled$samples$stage == &quot;sample&quot;]) # unwind function unwind &lt;- function(x, reverse = FALSE) { if (reverse) { n &lt;- sqrt(2 * length(x) + 0.25) - 0.5 ## Dimensions of matrix out &lt;- array(0, dim = c(n, n)) out[lower.tri(out, diag = TRUE)] = x diag(out) = exp(diag(out)) out = out%*%t(out) } else { y = t(chol(x)) diag(y) = log(diag(y)) out = y[lower.tri(y,diag=TRUE)] } return(out) } #unwound sigma pts2.unwound &lt;- apply(sig, 3, unwind) n.params &lt;- nrow(pts2.unwound) + n_randeffect + n_randeffect all_samples &lt;- array(dim = c(n_subjects, n.params, n_iter)) mu_tilde &lt;- array(dim = c(n_subjects, n.params)) sigma_tilde &lt;- array(dim = c(n_subjects, n.params, n.params)) for (j in 1:n_subjects){ all_samples[j,,] &lt;- rbind(alpha[,j,], theta[,], pts2.unwound[,]) # Calculate the mean for re, mu and sigma mu_tilde[j,] &lt;- apply(all_samples[j,,], 1, mean) # Calculate the covariance matrix for random effects, mu and sigma sigma_tilde[j,,] &lt;- cov(t(all_samples[j,,])) } X &lt;- cbind(t(theta), t(pts2.unwound), t(a_half)) 5.1.4 Estimate the normal mix Here we create an importance distribution by using a mixture of two Gaussian distributions, however this can be done differently. This takes ages. # do k=2, for a mixture of 2 gaussians # Number of distributions k &lt;- 2 #mvnormalmixEM is a weak point - function can fail. needs a note or output to show if it doesn&#39;t work. Should restart if it fails mix = NULL while(is.null(mix)) { tryCatch(mix&lt;-mvnormalmixEM(X,k=k, maxit = 5000),error=function(e){ },finally={}) } mix_weight &lt;- mix$lambda mix_mu &lt;- mix$mu mix_sigma &lt;- mix$sigma 5.1.5 Generate proposal parameters from importance samples Now that we have our importance distribution, we can generate proposals from this. Here, we protect against low amounts of samples by including the pmax and pmin arguments, ensuring that even if the weights are low, that we do sample from the both parts of the mixture. # Generate the proposal parameters from the mix of importance samples # Use the weight to get samples for n1. n2 = samples-n1 (i.e 9000 and 1000) n1 &lt;- rbinom(n=1, size = IS_samples, prob = max(mix_weight) ) n1 &lt;- pmax(n1, 2) n1 &lt;- pmin(n1, IS_samples - 2) n2 &lt;- IS_samples - n1 # Generate the 10,000 IS proposals given the mix proposals1 &lt;- rmvnorm(n1, mix_mu[[1]], mix_sigma[[1]] ) proposals2 &lt;- rmvnorm(n2, mix_mu[[2]], mix_sigma[[2]] ) prop_theta &lt;- rbind(proposals1, proposals2) 5.1.6 Write a group distribution function This function is used in the IS2 algorithm, however, it will vary with the type of priors that are set. For PMwG we use a multivariate normal prior and so here we calculate the density using dmvnorm. The density is calculated for the current random effect particle given the group level parameters and variance. groupdist &lt;- function(random_effect = NULL, parameters, sample = FALSE, n_samples = NULL, n_randeffect){ param.theta.mu &lt;- parameters[1:n_randeffect] # Scott would like it to ask for n(unwind) rather than doing the calculation # for how many it actually needs, you should just input the length of the unwound object param.theta.sig.unwound &lt;- parameters[(n_randeffect + 1):(length(parameters) - n_randeffect)] param.theta.sig2 &lt;- unwind(param.theta.sig.unwound, reverse = TRUE) if (sample){ return(mvtnorm::rmvnorm(n_samples, param.theta.mu, param.theta.sig2) ) }else{ logw_second &lt;- mvtnorm::dmvnorm(random_effect, param.theta.mu, param.theta.sig2, log = TRUE) return(logw_second) } } 5.1.7 Write a prior distribution function This function is used in PMwG to calculate the density under the prior. Here we use Huang and Wand’s (2013) prior (as used in PMwG) for a multivariate normal. The final line shows the value that is returned, which is equation x in the paper. This takes the density of the current parameters under the prior mean (log_prior_mu), variance (log_prior_sigma) and variance on the variance (log_prior_a). There are several other calculations performed here, which can be found in the paper. prior_dist &lt;- function(parameters, prior_parameters = sampled$prior, n_randeffect) { #mod notes: the sampled$prior needs to be fixed/passed in some other time param.theta.mu &lt;- parameters[1:n_randeffect] param.theta.sig.unwound &lt;- parameters[(n_randeffect+1):(length(parameters)-n_randeffect)] #scott would like it to ask for n(unwind) param.theta.sig2 &lt;- unwind(param.theta.sig.unwound, reverse = TRUE) param.a &lt;- exp(parameters[((length(parameters)-n_randeffect)+1):(length(parameters))]) v_alpha=2 log_prior_mu &lt;- mvtnorm::dmvnorm(param.theta.mu, mean = prior_parameters$theta_mu_mean, sigma = prior_parameters$theta_mu_var, log = TRUE) log_prior_sigma &lt;- log(MCMCpack::diwish(param.theta.sig2, v = v_alpha + n_randeffect-1, S = 2 * v_alpha * diag(1 / param.a))) # exp of a-half -&gt; positive only log_prior_a &lt;- sum(invgamma::dinvgamma(param.a, scale = 0.5, shape = 1, log = TRUE)) logw_den2 &lt;- sum(log(1 / param.a)) # Jacobian determinant of transformation of log of the a-half logw_den3 &lt;- log(2^n_randeffect) + sum((n_randeffect:1 + 1) * log(diag(param.theta.sig2))) #Jacobian determinant of Cholesky factors of covariance matrix return(log_prior_mu + log_prior_sigma + log_prior_a + logw_den3 - logw_den2) } 5.1.8 Write a get_logp function Next we need to create the function used to calculate the density of each particle. This is Equation 5 in the paper. This function calculates the density of each proposal for each subject across n particles. Here we first generate the particles from a mix of the group level parameters and a conditional multivariate normal, conditioned on the current subjects mean and variance. We then obtain the density of these proposed parameters under the likelihood and the group_dist functions over the likelihood of these proposals (as per equation 5). We then protect against badness and return the sum of these values (summed across participants, where participants are summed across particles) get_logp &lt;- function(prop_theta, data, n_subjects, n_particles, n_randeffect, mu_tilde, sigma_tilde, i, group_dist = group_dist) { #make an array for the density logp &lt;- array(dim = c(n_particles, n_subjects)) # for each subject, get 1000 IS samples (particles) and find log weight of each for (j in 1:n_subjects){ #generate the particles from the conditional MVnorm AND mix of group level proposals wmix &lt;- 0.95 n1 &lt;- rbinom(n = 1, size = n_particles, prob = wmix) if (n1 &lt; 2) n1 &lt;- 2 if (n1 &gt; (n_particles - 2)) n1 &lt;- n_particles - 2 ## These just avoid degenerate arrays. n2 &lt;- n_particles - n1 # do conditional MVnorm based on the proposal distribution conditional &lt;- condMVNorm::condMVN(mean = mu_tilde[j,], sigma = sigma_tilde[j,,], dependent.ind = 1:n_randeffect, given.ind = (n_randeffect + 1):n.params, X.given = prop_theta[i, 1:(n.params-n_randeffect)]) particles1 &lt;- mvtnorm::rmvnorm(n1, conditional$condMean, conditional$condVar) # mix of proposal parameters and conditional particles2 &lt;- group_dist(n_samples = n2, parameters = prop_theta[i,], sample = TRUE, n_randeffect = n_randeffect) particles &lt;- rbind(particles1, particles2) for (k in 1:n_particles){ x &lt;- particles[k,] #names for ll function to work #mod notes: this is the bit the prior effects names(x) &lt;- pars # do lba log likelihood with given parameters for each subject, gets density of particle from ll func logw_first &lt;- sampled$ll_func(x, data = data[as.numeric(factor(data$subject))==j,]) #mod notes: do we pass this in or the whole sampled object???? # below gets second part of equation 5 numerator i.e. density under prop_theta # particle k and big vector of things logw_second &lt;- group_dist(random_effect = particles[k,], parameters = prop_theta[i,], sample= FALSE, n_randeffect = n_randeffect) #mod notes: group dist # below is the denominator - i.e. mix of density under conditional and density under pro_theta logw_third &lt;- log(wmix*dmvnorm(particles[k,], conditional$condMean, conditional$condVar) + (1-wmix) * exp(logw_second)) #mod notes: fine? #does equation 5 logw = (logw_first + logw_second) - logw_third #assign to correct row/column logp[k,j] = logw } } #we use this part to centre the logw before adding back on at the end. This avoids inf and -inf values sub_max = apply(logp, 2, max) logw = t(t(logp) - sub_max) w = exp(logw) subj_logp = log(apply(w, 2, mean)) + sub_max #means # sum the logp and return return(sum(subj_logp)) } 5.1.9 Compute the Log Weights Finally, we need to create our compute_lw function. This function does equation 10 to obtain the log weights for the proposed particles. We first use get_logp to to get the density of the particles for each subject, then use prior_dist to get the density of the proposals under the prior. Finally we get the density of the proposed parameters under the mixture distribution. This then gives us equation 10. compute_lw &lt;- function(prop_theta, data, n_subjects, n_particles, n_randeffect, mu_tilde, sigma_tilde, i, prior_dist = prior_dist, sampled = sampled) { logp.out &lt;- get_logp(prop_theta, data, n_subjects, n_particles, n_randeffect, mu_tilde, sigma_tilde, i, group_dist = group_dist) ##do equation 10 logw_num &lt;- logp.out[1] + prior_dist(parameters = prop_theta[i,], prior_parameters = sampled$prior, n_randeffect ) logw_den &lt;- log(mix_weight[1] * mvtnorm::dmvnorm(prop_theta[i,], mix_mu[[1]], mix_sigma[[1]]) + mix_weight[2]* mvtnorm::dmvnorm(prop_theta[i,], mix_mu[[2]], mix_sigma[[2]]) ) #density of proposed params under the means logw &lt;- logw_num - logw_den #this is the equation 10 return(c(logw)) #NOTE: we should leave a note if variance is bad - variance is given by the logp function (currently commented out) } 5.1.10 Make it work Next we have to run it, see code below; #makes an array to store the IS samples tmp &lt;- array(dim = c(IS_samples)) #do the sampling if (cpus&gt;1){ tmp &lt;- mclapply(X=1:IS_samples, mc.cores = cpus, FUN = compute_lw, prop_theta = prop_theta, data = data, n_subjects= n_subjects, n_particles = n_particles, n_randeffect = n_randeffect, mu_tilde=mu_tilde, sigma_tilde = sigma_tilde, prior_dist = prior_dist, sampled = sampled) } else{ for (i in 1:IS_samples){ cat(i) tmp[i] &lt;- compute_lw(prop_theta, data, n_subjects, n_particles, n_randeffect, mu_tilde, sigma_tilde, i, prior_dist = prior_dist, sampled = sampled) } } # get the ML value finished &lt;- tmp tmp &lt;- unlist(tmp) max.lw &lt;- max(tmp) mean.centred.lw &lt;- mean(exp(tmp - max.lw)) #takes off the max and gets mean (avoids infinities) lw &lt;-log(mean.centred.lw) + max.lw #puts max back on to get the lw 5.1.11 Bootstrapping for SE To calculate standard error, we use a bootstrapping method, which can be done using the code below. bootstrap &lt;- 10000 log_marglik_boot &lt;- array(dim = bootstrap) for (i in 1:bootstrap){ log_weight_boot &lt;- sample(tmp, IS_samples, replace = TRUE) #resample with replacement from the lw max.boot &lt;- max(log_weight_boot) centred.boot &lt;- mean(exp(log_weight_boot - max.boot)) #takes off the max and gets mean (avoids infinities) log_marglik_boot[i] &lt;- log(centred.boot) + max.boot #puts max back on } var(log_marglik_boot) ###SE "],["troubleshoot.html", "Chapter 6 Troubleshooting PMwG errors 6.1 Writing your log-likelihood function: Tips, errors and check list", " Chapter 6 Troubleshooting PMwG errors 6.1 Writing your log-likelihood function: Tips, errors and check list 1. The parameter specified does not exist The parameter name is not specified to be estimated i.e. it is not in the parameter names argument or it is misspelled. Make sure pars vector contains the same parameter names you have included in your log-likelihood function and it is the same length. Do not rely on the log likelihood function to throw an error in this case. (e.g.x[‘b’]) 2. All non-continuous data frame variables must be a factor. Data frame variables should be factors unless the variable is a continuous variable e.g. response time. If you pass character variables to if statements and/or for loops in your log likelihood function, errors will not occur, however, your log likelihood estimate will be incorrect. For example, avoid using character strings like data$condition == “easy”. If you must use a character string, be sure to convert the string to a factor with as.factor. 3. Spelling errors or mismatched column name references Correctly reference data frame column names in your log likelihood function e.g. data$RT != data$rt. 4. When initialising a vector of parameter values - values are not filling in properly E.g. When a vector for b for all the values across the data set to be used, but there are NAs filling it somewhere. 5. Make sure operations are done on the right scale. 6. Data frame variables are scaled appropriately for the model Check your variables are correctly scaled and in the correct units. For example, with the LBA, response times must be in seconds rather than milliseconds. 7. The log-likelihood is printed/outputted at the end of function Make sure your log-likelihood function prints an estimate at the end of the function and the estimate is correctly obtained e.g. sum the log-likelihood estimates for all trials/rows. 8. Sampling error occurs When sampling, the generated columns are not outputted 9. When executing functions row by row (i.e. trial-wise), index MUST be included If writing a trial-wise/row-wise function (e.g. if statement, for loop), index i must be specified. if (data$condition == “easy”) # Incorrect reference when iterating over variable if (data$condition[i] == “easy”) # Include i index 10. Changing parameter values changes the log-likelihood estimate A simple check to run on your log-likelihood function is to modify your parameter values and observe the change to log-likelihood estimate. Then check if changing parameter values which rely on conditions actually change the log-likelihood estimate. 11. Make sure you have the latest version of the PMwG Samplers package checkVersion(“pmwg”) 12. Number of iterations for ’burn-in` stage of sampler We suggest running burn-in for few iterations and particles first. This will give you a sense of a) whether the sampler is working as intended (see troubleshooting/checks for what parameter chains should look like), b) the number of iterations &amp; particles needed to achieve the target acceptance rate, as well as the appropriate epsilon value. The acceptance rate is generally very high for the first few iterations ( &gt; 100) and then declines. After the initial short run, you can check and optimise the number of particles to be used (and balance with epsilon), so the acceptance rate is close to 30% on a longer, full run. We recommend you start with epsilon = .5 to increase efficiency, then adjust as needed. NOTE: Overall, we aim for ~30% acceptance rate of particles. High acceptance rates may be inaccurate if burn-in runs for few iterations. Low acceptance rates are inefficient and may fail to create an efficient distribution for the sampling stage. 13. My model is taking a lifetime to run If you’re running your models on a laptop and the run time is unreasonable, if possible, run your models on a computing grid with multiple CPU cores. "],["appendix.html", "Chapter 7 Appendix 7.1 Wagenmakers SDT script", " Chapter 7 Appendix 7.1 Wagenmakers SDT script Use the script below to process the data file so that the dataset structure is the same as shown in our SDT example. A zipped folder containing the dataset from Wagenmakers et al. (2008) can be found at this link. As shown in our script below, the data is stored in the \"PropData.txt\" file. #~~~~~~~~~~~~~~~~~~~~ Wagenmakers 2008 Lexical decision task ~~~~~~~~~~~~~~~~~~~~~~~~~~~# # 1) subject = participant number # 2) block = block number # 3) practice = 1 if practice block, otherwise 0 # 4) cond = condition either &quot;2&quot; for 75% words or &quot;1&quot; for 75% non-words) # 5) stimulus = unique identifier of stimulus, stimuli are nested in frequency conditions # 6) freq = Code &quot;1&quot; means &quot;high frequency word&quot;, code &quot;2&quot; means &quot;low frequency word&quot;, # and code &quot;3&quot; means &quot;very low frequency word&quot;. Codes 4, 5, and 6 = &quot;non-word&quot;. # 7) resp = 0 is non-word, 1 is word, -1 is not interpretable response (i.e., pushed a button, # but not the right one and also not the one next to the right button) # 8) rt = response time in seconds # 9) censor = 1 if value is eliminated from further analysis; # practice block, uninterpretable response, too fast response (&lt;180 ms), too slow response (&gt;3 sec) rm(list=ls()) require(tidyverse) wagenmakers2008 &lt;- read.delim(&quot;PropData.txt&quot;, header = FALSE) names(wagenmakers2008) &lt;- c(&quot;subject&quot;,&quot;block&quot;,&quot;practice&quot;, &quot;cond&quot;,&quot;stimulus&quot;,&quot;freq&quot;,&quot;resp&quot;,&quot;rt&quot;, &quot;censor&quot;) wagenmakers2008 &lt;- wagenmakers2008[wagenmakers2008$censor!=1,-c(3,9)] wagenmakers2008$subject &lt;- factor(wagenmakers2008$subject) wagenmakers2008$cond &lt;- factor(wagenmakers2008$cond, labels = c(&quot;nw&quot;,&quot;w&quot;)) wagenmakers2008$stimulus &lt;- wagenmakers2008$freq &lt; 4 wagenmakers2008$stimulus &lt;- factor(wagenmakers2008$stimulus, labels = c(&quot;nw&quot;, &quot;w&quot;)) wagenmakers2008$resp &lt;- factor(wagenmakers2008$resp, labels = c(&quot;NW&quot;,&quot;W&quot;)) wagenmakers2008$freq &lt;- ((wagenmakers2008$freq-1) %% 3)+1 wagenmakers2008$freq &lt;- factor(wagenmakers2008$freq, labels = c(&quot;hf&quot;,&quot;lf&quot;,&quot;vlf&quot;)) wagenmakers2008$correct &lt;- toupper(wagenmakers2008$stimulus) == toupper(wagenmakers2008$resp) wagenmakers2008$W &lt;- as.character(wagenmakers2008$freq) wagenmakers2008$W[wagenmakers2008$stimulus == &quot;nw&quot;] &lt;- &quot;nw&quot; wagenmakers2008$W &lt;- factor(wagenmakers2008$W, c(&quot;hf&quot;,&quot;lf&quot;,&quot;vlf&quot;,&quot;nw&quot;)) wagenmakers2008 &lt;- select(.data = wagenmakers2008, subject, cond, W, resp, rt, correct) %&gt;% rename(stimulus = W) wagenmakers2008$correct &lt;- if_else(wagenmakers2008$correct, true = &quot;2&quot;, false = &quot;1&quot;) save(wagenmakers2008, file = &quot;wagenmakers2008.RData&quot;) References Wagenmakers, Eric-Jan, Roger Ratcliff, Pablo Gomez, and Gail McKoon. 2008. “A Diffusion Model Account of Criterion Shifts in the Lexical Decision Task.” Journal of Memory and Language 58 (1): 140–59. "],["references.html", "References", " References Anderson, Nicole D. 2015. “Teaching Signal Detection Theory with Pseudoscience.” Frontiers in Psychology 6: 762. Brown, Scott, and Andrew Heathcote. 2008. “The Simplest Complete Model of Choice Response Time: Linear Ballistic Accumulation.” Cognitive Psychology 57 (3): 153–78. Forstmann, Birte U, Gilles Dutilh, Scott Brown, Jane Neumann, D Yves Von Cramon, K Richard Ridderinkhof, and Eric-Jan Wagenmakers. 2008. “Striatum and Pre-SMA Facilitate Decision-Making Under Time Pressure.” Proceedings of the National Academy of Sciences 105 (45): 17538–42. Shiffrin, Richard M, Michael D Lee, Woojae Kim, and Eric-Jan Wagenmakers. 2008. “A Survey of Model Evaluation Approaches with a Tutorial on Hierarchical Bayesian Methods.” Cognitive Science 32 (8): 1248–84. Van Ravenzwaaij, Don, Pete Cassey, and Scott D Brown. 2018. “A Simple Introduction to Markov Chain Monte–Carlo Sampling.” Psychonomic Bulletin &amp; Review 25 (1): 143–54. Wagenmakers, Eric-Jan, Roger Ratcliff, Pablo Gomez, and Gail McKoon. 2008. “A Diffusion Model Account of Criterion Shifts in the Lexical Decision Task.” Journal of Memory and Language 58 (1): 140–59. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
